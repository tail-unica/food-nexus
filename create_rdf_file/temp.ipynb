{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e94afd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rows off from 0 to 100000\n",
      "Processing rows off from 100000 to 200000\n",
      "Processing rows off from 200000 to 300000\n",
      "Processing rows off from 300000 to 400000\n",
      "Processing rows off from 400000 to 500000\n",
      "Processing rows off from 500000 to 600000\n",
      "Processing rows off from 600000 to 700000\n",
      "Processing rows off from 700000 to 800000\n",
      "Processing rows off from 800000 to 900000\n",
      "Processing rows off from 900000 to 1000000\n",
      "Processing rows off from 1000000 to 1100000\n",
      "Processing rows off from 1100000 to 1200000\n",
      "Processing rows off from 1200000 to 1300000\n",
      "Processing rows off from 1300000 to 1400000\n",
      "Processing rows off from 1400000 to 1500000\n",
      "Processing rows off from 1500000 to 1600000\n",
      "Processing rows off from 1600000 to 1700000\n",
      "Processing rows off from 1700000 to 1800000\n",
      "Processing rows off from 1800000 to 1900000\n",
      "Processing rows off from 1900000 to 2000000\n",
      "Processing rows off from 2000000 to 2100000\n",
      "Processing rows off from 2100000 to 2200000\n",
      "Processing rows off from 2200000 to 2300000\n",
      "Processing rows off from 2300000 to 2400000\n",
      "Processing rows off from 2400000 to 2500000\n",
      "Processing rows off from 2500000 to 2600000\n",
      "Processing rows off from 2600000 to 2700000\n",
      "Processing rows off from 2700000 to 2800000\n",
      "Processing rows off from 2800000 to 2900000\n",
      "Processing rows off from 2900000 to 3000000\n",
      "Processing rows off from 3000000 to 3100000\n",
      "Processing rows off from 3100000 to 3200000\n",
      "Processing rows off from 3200000 to 3300000\n",
      "Processing rows off from 3300000 to 3400000\n",
      "Processing rows off from 3400000 to 3500000\n",
      "Processing rows hummus from 0 to 100000\n",
      "Processing rows hummus from 100000 to 200000\n",
      "Processing rows hummus from 200000 to 300000\n",
      "Processing rows hummus from 300000 to 400000\n",
      "Processing rows hummus from 400000 to 500000\n",
      "Processing rows hummus from 500000 to 600000\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from rdflib import RDF, RDFS, XSD, Graph, Literal, Namespace, URIRef\n",
    "from rdflib.namespace import OWL\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import ast\n",
    "import gc \n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def sanitize_for_uri(value) -> str:\n",
    "    \"\"\"\n",
    "    Generic sanitization function for URIs\n",
    "\n",
    "    :param value: value to sanitize\n",
    "\n",
    "    :return: sanitized value\n",
    "    \"\"\"\n",
    "    return re.sub(r\"[^a-zA-Z0-9_]\", \"\", str(value))\n",
    "\n",
    "UNICA = Namespace(\"https://github.com/tail-unica/kgeats/\")\n",
    "SCHEMA = Namespace(\"https://schema.org/\")\n",
    "\n",
    "dizionario_hum = {}\n",
    "dizionario_off = {}\n",
    "\n",
    "hum_file = \"../csv_file/pp_recipes_normalized_by_pipeline.csv\"\n",
    "off_file = \"../csv_file/off_normalized_final.csv\"\n",
    "hum_off_file = \"../csv_file/file_off_hummus_filtered_90.csv\"\n",
    "file_output_nt =  \"../csv_file/ontology_merge.nt\"\n",
    "\n",
    "chunksize = 100000\n",
    "cont_chunk = 0\n",
    "\n",
    "for df_off_chunk in pd.read_csv(off_file, sep=\"\\t\", on_bad_lines=\"skip\", chunksize=chunksize, low_memory=False, usecols=[\"product_name_normalized\", \"code\"]):\n",
    "    print(f\"Processing rows off from {chunksize * cont_chunk} to {chunksize * (cont_chunk+1)}\")\n",
    "    \n",
    "    for idx, row in df_off_chunk.iterrows():\n",
    "        if(row[\"product_name_normalized\"] != None and row[\"product_name_normalized\"] != \"\"):\n",
    "            id = URIRef(value=UNICA[f\"Recipe_off_{row[\"code\"]}\"])\n",
    "            if id != None:\n",
    "                if row[\"product_name_normalized\"] not in dizionario_off:\n",
    "                    dizionario_off[row[\"product_name_normalized\"]] = [id]\n",
    "                else: \n",
    "                    dizionario_off[row[\"product_name_normalized\"]].append(id)\n",
    "    cont_chunk += 1\n",
    "\n",
    "cont_chunk = 0\n",
    "for df_hum_chunk in pd.read_csv(hum_file, sep=\";\", on_bad_lines=\"skip\", chunksize=chunksize, low_memory=False, usecols=[\"title_normalized\", \"recipe_id\"]):\n",
    "    print(f\"Processing rows hummus from {chunksize * cont_chunk} to {chunksize * (cont_chunk+1)}\")\n",
    "    \n",
    "    for idx, row in df_hum_chunk.iterrows():\n",
    "        if(row[\"title_normalized\"] != None and row[\"title_normalized\"] != \"\"):\n",
    "            id = URIRef(UNICA[f\"Recipe_hummus{sanitize_for_uri(row['recipe_id'])}\"])\n",
    "            if id != None:\n",
    "                if row[\"title_normalized\"] not in dizionario_hum:\n",
    "                    dizionario_hum[row[\"title_normalized\"]] = [id]\n",
    "                else: \n",
    "                    dizionario_hum[row[\"title_normalized\"]].append(id)\n",
    "    cont_chunk += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ad9c790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunk 1/31\n",
      "Chunk time: 14.78s — Estimated remaining: 7.4 min\n",
      "135266849\n",
      "\n",
      "Processing chunk 2/31\n",
      "Chunk time: 4.46s — Estimated remaining: 4.7 min\n",
      "160939227\n",
      "\n",
      "Processing chunk 3/31\n",
      "Chunk time: 3.83s — Estimated remaining: 3.6 min\n",
      "179020615\n",
      "\n",
      "Processing chunk 4/31\n",
      "Chunk time: 2.10s — Estimated remaining: 2.9 min\n",
      "182462504\n",
      "\n",
      "Processing chunk 5/31\n",
      "Chunk time: 2.29s — Estimated remaining: 2.4 min\n",
      "187008640\n",
      "\n",
      "Processing chunk 6/31\n",
      "Chunk time: 2.26s — Estimated remaining: 2.1 min\n",
      "191990185\n",
      "\n",
      "Processing chunk 7/31\n",
      "Chunk time: 1.96s — Estimated remaining: 1.9 min\n",
      "194986320\n",
      "\n",
      "Processing chunk 8/31\n",
      "Chunk time: 2.27s — Estimated remaining: 1.7 min\n",
      "199894707\n",
      "\n",
      "Processing chunk 9/31\n",
      "Chunk time: 2.33s — Estimated remaining: 1.5 min\n",
      "205644268\n",
      "\n",
      "Processing chunk 10/31\n",
      "Chunk time: 1.90s — Estimated remaining: 1.4 min\n",
      "207859583\n",
      "\n",
      "Processing chunk 11/31\n",
      "Chunk time: 2.18s — Estimated remaining: 1.3 min\n",
      "211101578\n",
      "\n",
      "Processing chunk 12/31\n",
      "Chunk time: 1.82s — Estimated remaining: 1.2 min\n",
      "212035623\n",
      "\n",
      "Processing chunk 13/31\n",
      "Chunk time: 2.03s — Estimated remaining: 1.1 min\n",
      "214358792\n",
      "\n",
      "Processing chunk 14/31\n",
      "Chunk time: 1.97s — Estimated remaining: 1.0 min\n",
      "215168776\n",
      "\n",
      "Processing chunk 15/31\n",
      "Chunk time: 2.16s — Estimated remaining: 0.9 min\n",
      "218582021\n",
      "\n",
      "Processing chunk 16/31\n",
      "Chunk time: 2.31s — Estimated remaining: 0.8 min\n",
      "222042166\n",
      "\n",
      "Processing chunk 17/31\n",
      "Chunk time: 2.17s — Estimated remaining: 0.8 min\n",
      "224153325\n",
      "\n",
      "Processing chunk 18/31\n",
      "Chunk time: 2.03s — Estimated remaining: 0.7 min\n",
      "225638995\n",
      "\n",
      "Processing chunk 19/31\n",
      "Chunk time: 2.17s — Estimated remaining: 0.6 min\n",
      "227033056\n",
      "\n",
      "Processing chunk 20/31\n",
      "Chunk time: 1.98s — Estimated remaining: 0.6 min\n",
      "228198691\n",
      "\n",
      "Processing chunk 21/31\n",
      "Chunk time: 1.98s — Estimated remaining: 0.5 min\n",
      "229867692\n",
      "\n",
      "Processing chunk 22/31\n",
      "Chunk time: 2.12s — Estimated remaining: 0.4 min\n",
      "231430665\n",
      "\n",
      "Processing chunk 23/31\n",
      "Chunk time: 2.03s — Estimated remaining: 0.4 min\n",
      "232707463\n",
      "\n",
      "Processing chunk 24/31\n",
      "Chunk time: 1.94s — Estimated remaining: 0.3 min\n",
      "233994335\n",
      "\n",
      "Processing chunk 25/31\n",
      "Chunk time: 1.88s — Estimated remaining: 0.3 min\n",
      "234875329\n",
      "\n",
      "Processing chunk 26/31\n",
      "Chunk time: 2.09s — Estimated remaining: 0.2 min\n",
      "236364360\n",
      "\n",
      "Processing chunk 27/31\n",
      "Chunk time: 2.00s — Estimated remaining: 0.2 min\n",
      "237491294\n",
      "\n",
      "Processing chunk 28/31\n",
      "Chunk time: 1.94s — Estimated remaining: 0.1 min\n",
      "238574972\n",
      "\n",
      "Processing chunk 29/31\n",
      "Chunk time: 2.08s — Estimated remaining: 0.1 min\n",
      "239518332\n",
      "\n",
      "Processing chunk 30/31\n",
      "Chunk time: 2.00s — Estimated remaining: 0.0 min\n",
      "240541375\n",
      "\n",
      "Processing chunk 31/31\n",
      "Chunk time: 1.86s — Estimated remaining: 0.0 min\n",
      "241323239\n",
      "\n",
      "Total processing time: 1.41 minutes\n"
     ]
    }
   ],
   "source": [
    "numchunk = 0\n",
    "chunksize = 300000\n",
    "contatore = 0\n",
    "hum_off_file = \"../csv_file/file_off_hummus_filtered_875.csv\"\n",
    "\n",
    "total_lines = sum(1 for _ in open(hum_off_file, encoding=\"utf-8\")) - 1\n",
    "total_chunks = (total_lines // chunksize) + 1\n",
    "start_total = time.time()\n",
    "\n",
    "\n",
    "for df_merge_chunk in pd.read_csv(hum_off_file, sep=\",\", on_bad_lines=\"skip\", chunksize=chunksize, low_memory=False, usecols=[\"title_normalized\", \"product_name_normalized\"]):\n",
    "    chunk_start = time.time()\n",
    "    print(f\"\\nProcessing chunk {numchunk+1}/{total_chunks}\")\n",
    "\n",
    "    for row in df_merge_chunk.itertuples(index=False):\n",
    "        title = row.title_normalized\n",
    "        product = row.product_name_normalized\n",
    "\n",
    "        if title in dizionario_hum and product in dizionario_off:\n",
    "            for hum_ricetta in dizionario_hum[title]:\n",
    "                massimo_ricette = 1000\n",
    "                for off_ricetta in dizionario_off[product]: \n",
    "                    contatore += 1\n",
    "                    massimo_ricette -= 1\n",
    "                    if massimo_ricette == 0:\n",
    "                        break\n",
    "\n",
    "    del df_merge_chunk\n",
    "    gc.collect() \n",
    "\n",
    "    chunk_time = time.time() - chunk_start\n",
    "    avg_time_per_chunk = (time.time() - start_total) / (numchunk + 1)\n",
    "    remaining_chunks = total_chunks - (numchunk + 1)\n",
    "    est_remaining = avg_time_per_chunk * remaining_chunks\n",
    "    print(f\"Chunk time: {chunk_time:.2f}s — Estimated remaining: {est_remaining/60:.1f} min\")\n",
    "    print(contatore)\n",
    "    numchunk += 1\n",
    "\n",
    "total_time = time.time() - start_total\n",
    "print(f\"\\nTotal processing time: {total_time/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61821d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunk 1/12374\n",
      "Chunk time: 360.23s — Estimated remaining: 74288.0 min\n",
      "\n",
      "Processing chunk 101/12374\n",
      "Chunk time: 1.15s — Estimated remaining: 1006.0 min\n",
      "\n",
      "Processing chunk 201/12374\n",
      "Chunk time: 1.04s — Estimated remaining: 612.0 min\n",
      "\n",
      "Processing chunk 301/12374\n",
      "Chunk time: 1.13s — Estimated remaining: 480.2 min\n",
      "\n",
      "Processing chunk 401/12374\n",
      "Chunk time: 1.12s — Estimated remaining: 413.2 min\n",
      "\n",
      "Processing chunk 501/12374\n",
      "Chunk time: 1.03s — Estimated remaining: 369.5 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fc491033230>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gzedda/miniconda3/envs/ambientez/lib/python3.13/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "numchunk = 0\n",
    "chunksize = 10\n",
    "\n",
    "total_lines = sum(1 for _ in open(hum_off_file, encoding=\"utf-8\")) - 1\n",
    "total_chunks = (total_lines // chunksize) + 1\n",
    "start_total = time.time()\n",
    "\n",
    "with open(file_output_nt, \"w\", encoding=\"utf-8\") as f_out:\n",
    "\n",
    "    for df_merge_chunk in pd.read_csv(hum_off_file, sep=\",\", on_bad_lines=\"skip\", chunksize=chunksize, low_memory=False, usecols=[\"title_normalized\", \"product_name_normalized\"]):\n",
    "        chunk_start = time.time()\n",
    "        if numchunk % 100 == 0:\n",
    "            print(f\"\\nProcessing chunk {numchunk+1}/{total_chunks}\")\n",
    "\n",
    "        for row in df_merge_chunk.itertuples(index=False):\n",
    "            title = row.title_normalized\n",
    "            product = row.product_name_normalized\n",
    "\n",
    "            if title in dizionario_hum and product in dizionario_off:\n",
    "                for hum_ricetta in dizionario_hum[title]:\n",
    "                    for off_ricetta in dizionario_off[product]: \n",
    "                        triple_str = f\"<{off_ricetta}> <https://schema.org/sameAs> <{hum_ricetta}> .\\n\"\n",
    "                        f_out.write(triple_str)\n",
    "\n",
    "        del df_merge_chunk\n",
    "        gc.collect() \n",
    "\n",
    "        chunk_time = time.time() - chunk_start\n",
    "        avg_time_per_chunk = (time.time() - start_total) / (numchunk + 1)\n",
    "        remaining_chunks = total_chunks - (numchunk + 1)\n",
    "        est_remaining = avg_time_per_chunk * remaining_chunks\n",
    "        if numchunk % 100 == 0:\n",
    "            print(f\"Chunk time: {chunk_time:.2f}s — Estimated remaining: {est_remaining/60:.1f} min\")\n",
    "        numchunk += 1\n",
    "\n",
    "    total_time = time.time() - start_total\n",
    "    print(f\"\\nTotal processing time: {total_time/60:.2f} minutes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambientez",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
