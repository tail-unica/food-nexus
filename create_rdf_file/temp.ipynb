{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94afd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rows off from 0 to 100000\n",
      "Processing rows off from 100000 to 200000\n",
      "Processing rows off from 200000 to 300000\n",
      "Processing rows off from 300000 to 400000\n",
      "Processing rows off from 400000 to 500000\n",
      "Processing rows off from 500000 to 600000\n",
      "Processing rows off from 600000 to 700000\n",
      "Processing rows off from 700000 to 800000\n",
      "Processing rows off from 800000 to 900000\n",
      "Processing rows off from 900000 to 1000000\n",
      "Processing rows off from 1000000 to 1100000\n",
      "Processing rows off from 1100000 to 1200000\n",
      "Processing rows off from 1200000 to 1300000\n",
      "Processing rows off from 1300000 to 1400000\n",
      "Processing rows off from 1400000 to 1500000\n",
      "Processing rows off from 1500000 to 1600000\n",
      "Processing rows off from 1600000 to 1700000\n",
      "Processing rows off from 1700000 to 1800000\n",
      "Processing rows off from 1800000 to 1900000\n",
      "Processing rows off from 1900000 to 2000000\n",
      "Processing rows off from 2000000 to 2100000\n",
      "Processing rows off from 2100000 to 2200000\n",
      "Processing rows off from 2200000 to 2300000\n",
      "Processing rows off from 2300000 to 2400000\n",
      "Processing rows off from 2400000 to 2500000\n",
      "Processing rows off from 2500000 to 2600000\n",
      "Processing rows off from 2600000 to 2700000\n",
      "Processing rows off from 2700000 to 2800000\n",
      "Processing rows off from 2800000 to 2900000\n",
      "Processing rows off from 2900000 to 3000000\n",
      "Processing rows off from 3000000 to 3100000\n",
      "Processing rows off from 3100000 to 3200000\n",
      "Processing rows off from 3200000 to 3300000\n",
      "Processing rows off from 3300000 to 3400000\n",
      "Processing rows off from 3400000 to 3500000\n",
      "Processing rows hummus from 0 to 100000\n",
      "Processing rows hummus from 100000 to 200000\n",
      "Processing rows hummus from 200000 to 300000\n",
      "Processing rows hummus from 300000 to 400000\n",
      "Processing rows hummus from 400000 to 500000\n",
      "Processing rows hummus from 500000 to 600000\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from rdflib import RDF, RDFS, XSD, Graph, Literal, Namespace, URIRef\n",
    "from rdflib.namespace import OWL\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import ast\n",
    "import gc \n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def sanitize_for_uri(value) -> str:\n",
    "    \"\"\"\n",
    "    Generic sanitization function for URIs\n",
    "\n",
    "    :param value: value to sanitize\n",
    "\n",
    "    :return: sanitized value\n",
    "    \"\"\"\n",
    "    return re.sub(r\"[^a-zA-Z0-9_]\", \"\", str(value))\n",
    "\n",
    "UNICA = Namespace(\"https://github.com/tail-unica/kgeats/\")\n",
    "SCHEMA = Namespace(\"https://schema.org/\")\n",
    "\n",
    "dizionario_hum = {}\n",
    "dizionario_off = {}\n",
    "\n",
    "hum_file = \"../csv_file/pp_recipes_normalized_by_pipeline.csv\"\n",
    "off_file = \"../csv_file/off_normalized_final.csv\"\n",
    "hum_off_file = \"../csv_file/file_off_hummus.csv\"\n",
    "file_output_nt =  \"../csv_file/ontology_merge.nt\"\n",
    "\n",
    "chunksize = 100000\n",
    "cont_chunk = 0\n",
    "\n",
    "for df_off_chunk in pd.read_csv(off_file, sep=\"\\t\", on_bad_lines=\"skip\", chunksize=chunksize, low_memory=False, usecols=[\"product_name_normalized\", \"code\"]):\n",
    "    print(f\"Processing rows off from {chunksize * cont_chunk} to {chunksize * (cont_chunk+1)}\")\n",
    "    \n",
    "    for idx, row in df_off_chunk.iterrows():\n",
    "        if(row[\"product_name_normalized\"] != None and row[\"product_name_normalized\"] != \"\"):\n",
    "            id = URIRef(value=UNICA[f\"Recipe_off_{row[\"code\"]}\"])\n",
    "            if id != None:\n",
    "                if row[\"product_name_normalized\"] not in dizionario_off:\n",
    "                    dizionario_off[row[\"product_name_normalized\"]] = [id]\n",
    "                else: \n",
    "                    dizionario_off[row[\"product_name_normalized\"]].append(id)\n",
    "    cont_chunk += 1\n",
    "\n",
    "cont_chunk = 0\n",
    "for df_hum_chunk in pd.read_csv(hum_file, sep=\";\", on_bad_lines=\"skip\", chunksize=chunksize, low_memory=False, usecols=[\"title_normalized\", \"recipe_id\"]):\n",
    "    print(f\"Processing rows hummus from {chunksize * cont_chunk} to {chunksize * (cont_chunk+1)}\")\n",
    "    \n",
    "    for idx, row in df_hum_chunk.iterrows():\n",
    "        if(row[\"title_normalized\"] != None and row[\"title_normalized\"] != \"\"):\n",
    "            id = URIRef(UNICA[f\"Recipe_hummus{sanitize_for_uri(row['recipe_id'])}\"])\n",
    "            if id != None:\n",
    "                if row[\"title_normalized\"] not in dizionario_hum:\n",
    "                    dizionario_hum[row[\"title_normalized\"]] = [id]\n",
    "                else: \n",
    "                    dizionario_hum[row[\"title_normalized\"]].append(id)\n",
    "    cont_chunk += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad9c790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunk 1/3779657\n",
      "Chunk time: 1.32s — Estimated remaining: 83530.6 min\n",
      "564333\n",
      "\n",
      "Processing chunk 11/3779657\n",
      "Chunk time: 1.29s — Estimated remaining: 81831.1 min\n",
      "1076368\n",
      "\n",
      "Processing chunk 21/3779657\n",
      "Chunk time: 1.30s — Estimated remaining: 184634.7 min\n",
      "565903354\n",
      "\n",
      "Processing chunk 31/3779657\n",
      "Chunk time: 1.29s — Estimated remaining: 151398.6 min\n",
      "566181524\n",
      "\n",
      "Processing chunk 41/3779657\n",
      "Chunk time: 1.31s — Estimated remaining: 134532.7 min\n",
      "568785803\n",
      "\n",
      "Processing chunk 51/3779657\n",
      "Chunk time: 1.30s — Estimated remaining: 124213.4 min\n",
      "570103708\n",
      "\n",
      "Processing chunk 61/3779657\n",
      "Chunk time: 1.29s — Estimated remaining: 117217.8 min\n",
      "570408113\n",
      "\n",
      "Processing chunk 71/3779657\n",
      "Chunk time: 1.29s — Estimated remaining: 112194.9 min\n",
      "570825646\n",
      "\n",
      "Processing chunk 81/3779657\n",
      "Chunk time: 1.29s — Estimated remaining: 108406.0 min\n",
      "570869486\n",
      "\n",
      "Processing chunk 91/3779657\n",
      "Chunk time: 1.29s — Estimated remaining: 105453.3 min\n",
      "570871354\n",
      "\n",
      "Processing chunk 101/3779657\n",
      "Chunk time: 1.30s — Estimated remaining: 103091.2 min\n",
      "571010366\n",
      "\n",
      "Processing chunk 111/3779657\n",
      "Chunk time: 1.31s — Estimated remaining: 101239.7 min\n",
      "573785214\n",
      "\n",
      "Processing chunk 121/3779657\n",
      "Chunk time: 1.29s — Estimated remaining: 99622.6 min\n",
      "573976067\n",
      "\n",
      "Processing chunk 131/3779657\n",
      "Chunk time: 1.29s — Estimated remaining: 98251.1 min\n",
      "574255659\n",
      "\n",
      "Processing chunk 141/3779657\n",
      "Chunk time: 1.30s — Estimated remaining: 97090.3 min\n",
      "575275018\n",
      "\n",
      "Processing chunk 151/3779657\n",
      "Chunk time: 1.30s — Estimated remaining: 96098.5 min\n",
      "576924384\n",
      "\n",
      "Processing chunk 161/3779657\n",
      "Chunk time: 1.31s — Estimated remaining: 95232.6 min\n",
      "576949066\n",
      "\n",
      "Processing chunk 171/3779657\n",
      "Chunk time: 1.31s — Estimated remaining: 94469.6 min\n",
      "576999672\n",
      "\n",
      "Processing chunk 181/3779657\n",
      "Chunk time: 1.30s — Estimated remaining: 93784.9 min\n",
      "577031187\n",
      "\n",
      "Processing chunk 191/3779657\n",
      "Chunk time: 1.25s — Estimated remaining: 93115.8 min\n",
      "577354020\n",
      "\n",
      "Processing chunk 201/3779657\n",
      "Chunk time: 1.14s — Estimated remaining: 92202.1 min\n",
      "577923869\n",
      "\n",
      "Processing chunk 211/3779657\n",
      "Chunk time: 1.20s — Estimated remaining: 91332.1 min\n",
      "583608842\n",
      "\n",
      "Processing chunk 221/3779657\n",
      "Chunk time: 1.15s — Estimated remaining: 90601.9 min\n",
      "584453580\n",
      "\n",
      "Processing chunk 231/3779657\n",
      "Chunk time: 1.29s — Estimated remaining: 90179.6 min\n",
      "586055777\n",
      "\n",
      "Processing chunk 241/3779657\n",
      "Chunk time: 1.17s — Estimated remaining: 89710.6 min\n",
      "586182286\n",
      "\n",
      "Processing chunk 251/3779657\n",
      "Chunk time: 1.17s — Estimated remaining: 89076.7 min\n",
      "586220684\n",
      "\n",
      "Processing chunk 261/3779657\n",
      "Chunk time: 1.17s — Estimated remaining: 88487.2 min\n",
      "586305602\n",
      "\n",
      "Processing chunk 271/3779657\n",
      "Chunk time: 1.19s — Estimated remaining: 88013.3 min\n",
      "589361836\n",
      "\n",
      "Processing chunk 281/3779657\n",
      "Chunk time: 1.19s — Estimated remaining: 87556.8 min\n",
      "591514075\n",
      "\n",
      "Processing chunk 291/3779657\n",
      "Chunk time: 1.22s — Estimated remaining: 87186.1 min\n",
      "597830271\n",
      "\n",
      "Processing chunk 301/3779657\n",
      "Chunk time: 1.18s — Estimated remaining: 86779.3 min\n",
      "599112179\n",
      "\n",
      "Processing chunk 311/3779657\n",
      "Chunk time: 1.19s — Estimated remaining: 86394.8 min\n",
      "600481247\n",
      "\n",
      "Processing chunk 321/3779657\n",
      "Chunk time: 1.26s — Estimated remaining: 86041.4 min\n",
      "602687915\n",
      "\n",
      "Processing chunk 331/3779657\n",
      "Chunk time: 1.18s — Estimated remaining: 85690.1 min\n",
      "603084927\n",
      "\n",
      "Processing chunk 341/3779657\n",
      "Chunk time: 1.21s — Estimated remaining: 85374.4 min\n",
      "604949869\n",
      "\n",
      "Processing chunk 351/3779657\n",
      "Chunk time: 1.19s — Estimated remaining: 85077.0 min\n",
      "606502706\n",
      "\n",
      "Processing chunk 361/3779657\n",
      "Chunk time: 1.18s — Estimated remaining: 84788.0 min\n",
      "607421566\n",
      "\n",
      "Processing chunk 371/3779657\n",
      "Chunk time: 1.18s — Estimated remaining: 84520.5 min\n",
      "608971672\n",
      "\n",
      "Processing chunk 381/3779657\n",
      "Chunk time: 1.26s — Estimated remaining: 84381.6 min\n",
      "615662988\n",
      "\n",
      "Processing chunk 391/3779657\n",
      "Chunk time: 1.19s — Estimated remaining: 84234.9 min\n",
      "615973077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fe0566ef230>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gzedda/miniconda3/envs/ambientez/lib/python3.13/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunk 401/3779657\n",
      "Chunk time: 1.31s — Estimated remaining: 83988.5 min\n",
      "618372961\n"
     ]
    }
   ],
   "source": [
    "numchunk = 0\n",
    "chunksize = 10\n",
    "contatore = 0\n",
    "\n",
    "total_lines = sum(1 for _ in open(hum_off_file, encoding=\"utf-8\")) - 1\n",
    "total_chunks = (total_lines // chunksize) + 1\n",
    "start_total = time.time()\n",
    "\n",
    "\n",
    "for df_merge_chunk in pd.read_csv(hum_off_file, sep=\",\", on_bad_lines=\"skip\", chunksize=chunksize, low_memory=False, usecols=[\"title_normalized\", \"product_name_normalized\"]):\n",
    "    chunk_start = time.time()\n",
    "    if numchunk % 10 == 0:\n",
    "        print(f\"\\nProcessing chunk {numchunk+1}/{total_chunks}\")\n",
    "\n",
    "    for row in df_merge_chunk.itertuples(index=False):\n",
    "        title = row.title_normalized\n",
    "        product = row.product_name_normalized\n",
    "\n",
    "        if title in dizionario_hum and product in dizionario_off:\n",
    "            for hum_ricetta in dizionario_hum[title]:\n",
    "                for off_ricetta in dizionario_off[product]: \n",
    "                    contatore += 1\n",
    "\n",
    "    del df_merge_chunk\n",
    "    gc.collect() \n",
    "\n",
    "    chunk_time = time.time() - chunk_start\n",
    "    avg_time_per_chunk = (time.time() - start_total) / (numchunk + 1)\n",
    "    remaining_chunks = total_chunks - (numchunk + 1)\n",
    "    est_remaining = avg_time_per_chunk * remaining_chunks\n",
    "    if numchunk % 10 == 0:\n",
    "        print(f\"Chunk time: {chunk_time:.2f}s — Estimated remaining: {est_remaining/60:.1f} min\")\n",
    "        print(contatore)\n",
    "    numchunk += 1\n",
    "\n",
    "total_time = time.time() - start_total\n",
    "print(f\"\\nTotal processing time: {total_time/60:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61821d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunk 1/6\n",
      "Chunk time: 1.29s — Estimated remaining: 0.1 min\n",
      "\n",
      "Total processing time: 0.11 minutes\n"
     ]
    }
   ],
   "source": [
    "numchunk = 0\n",
    "chunksize = 1\n",
    "\n",
    "total_lines = 5 #sum(1 for _ in open(hum_off_file, encoding=\"utf-8\")) - 1\n",
    "total_chunks = (total_lines // chunksize) + 1\n",
    "start_total = time.time()\n",
    "\n",
    "with open(file_output_nt, \"w\", encoding=\"utf-8\") as f_out:\n",
    "\n",
    "    for df_merge_chunk in pd.read_csv(hum_off_file, sep=\",\", on_bad_lines=\"skip\", chunksize=chunksize, low_memory=False, usecols=[\"title_normalized\", \"product_name_normalized\"], nrows=5):\n",
    "        chunk_start = time.time()\n",
    "        if numchunk % 100 == 0:\n",
    "            print(f\"\\nProcessing chunk {numchunk+1}/{total_chunks}\")\n",
    "\n",
    "        for row in df_merge_chunk.itertuples(index=False):\n",
    "            title = row.title_normalized\n",
    "            product = row.product_name_normalized\n",
    "\n",
    "            if title in dizionario_hum and product in dizionario_off:\n",
    "                for hum_ricetta in dizionario_hum[title]:\n",
    "                    for off_ricetta in dizionario_off[product]: \n",
    "                        triple_str = f\"<{off_ricetta}> <https://schema.org/sameAs> <{hum_ricetta}> .\\n\"\n",
    "                        f_out.write(triple_str)\n",
    "\n",
    "        del df_merge_chunk\n",
    "        gc.collect() \n",
    "\n",
    "        chunk_time = time.time() - chunk_start\n",
    "        avg_time_per_chunk = (time.time() - start_total) / (numchunk + 1)\n",
    "        remaining_chunks = total_chunks - (numchunk + 1)\n",
    "        est_remaining = avg_time_per_chunk * remaining_chunks\n",
    "        if numchunk % 100 == 0:\n",
    "            print(f\"Chunk time: {chunk_time:.2f}s — Estimated remaining: {est_remaining/60:.1f} min\")\n",
    "        numchunk += 1\n",
    "\n",
    "    total_time = time.time() - start_total\n",
    "    print(f\"\\nTotal processing time: {total_time/60:.2f} minutes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambientez",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
