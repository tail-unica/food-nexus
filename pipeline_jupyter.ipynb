{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "needed nltk components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this code before activate the pipeline for the first time\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create normalization of Foodkg ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import pipeline #type: ignore\n",
    "\n",
    "file_input = \"csv_file/ingredients_food_kg.csv\"\n",
    "file_output = \"csv_file/ingredients_food_kg_normalizzed_by_pipeline.csv\"\n",
    "column_to_normalize = \"ingredient\"\n",
    "column_normalized = 'ingredient_normalized'\n",
    "pipeline(\n",
    "    input_file=file_input, output_file=file_output, show_something=True, show_all=False, column_name=column_to_normalize, new_column_name=column_normalized\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create normalization of hummus recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import pipeline #type: ignore\n",
    "\n",
    "input_file = \"csv_file/pp_recipes.csv\"\n",
    "output_file = \"csv_file/pp_recipes_normalized_by_pipeline.csv\"\n",
    "column_to_normalize = \"title\"\n",
    "column_normalized = 'title_normalized'\n",
    "pipeline(\n",
    "    input_file=input_file, output_file=output_file, show_something=True, show_all=True, column_name=column_to_normalize, new_column_name=column_normalized\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create normalization of off products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import pipeline #type: ignore\n",
    "\n",
    "input_file = \"csv_file/off_english.csv\"\n",
    "#input_file = \"csv_file/off_rows.csv\"\n",
    "output_file = \"csv_file/off_normalized_by_pipeline.csv\"\n",
    "column_to_normalize = \"product_name\"\n",
    "column_normalized = 'product_name_normalized'\n",
    "pipeline(\n",
    "    input_file=input_file, output_file=output_file, show_something=True, show_all=False, column_name=column_to_normalize, delimiter=\"\\t\", new_column_name=column_normalized\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create normalizzation for the entity link test file (step1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import pipeline #type: ignore\n",
    "\n",
    "input_file = 'csv_file/entity_linking_test.csv'  \n",
    "output_file = 'csv_file/entity_linking_test_normalized_temp.csv' \n",
    "column_to_normalize = 'off'\n",
    "column_normalized = 'off_normalized'\n",
    "pipeline(input_file=input_file, output_file=output_file, show_something=True, show_all=False, column_name=column_to_normalize, delimiter=',', new_column_name=column_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(step2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import pipeline #type: ignore\n",
    "\n",
    "input_file = 'csv_file/entity_linking_test_normalized_temp.csv'  \n",
    "output_file = 'csv_file/entity_linking_test_normalized.csv' \n",
    "column_to_normalize = 'foodkg'\n",
    "column_normalized = 'foodkg_normalized'\n",
    "pipeline(input_file=input_file, output_file=output_file, show_something=True, show_all=False, column_name=column_to_normalize, delimiter=',', new_column_name=column_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "infere information about the user from member description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import add_user_attributes #type: ignore\n",
    "\n",
    "column_name = \"member_description\"\n",
    "new_column_names = [\"weight\", \"height\", \"gender\", \"age\", \"user_constraints\"]\n",
    "input_file = \"csv_file/pp_members.csv\"\n",
    "output_file = \"csv_file/pp_members_with_attributes.csv\"\n",
    "\n",
    "add_user_attributes(\n",
    "    input_file=input_file,\n",
    "    output_file=output_file,\n",
    "    column_name=column_name,\n",
    "    new_column_names=new_column_names,\n",
    "    delimiter=\",\",\n",
    "    delimiter2=\",\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "infere information about the user from his reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import add_user_attributes #type: ignore\n",
    "\n",
    "column_name = \"text\"\n",
    "new_column_names = [\"weight\", \"height\", \"gender\", \"age\", \"user_constraints\"]\n",
    "input_file = \"csv_file/pp_reviews.csv\"\n",
    "output_file = \"csv_file/pp_reviews_normalized.csv\"\n",
    "\n",
    "add_user_attributes(\n",
    "    input_file=input_file,\n",
    "    output_file=output_file,\n",
    "    column_name=column_name,\n",
    "    new_column_names=new_column_names,\n",
    "    delimiter=\",\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcolate the cohen kappa score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import pipeline #type: ignore\n",
    "\n",
    "input_file = \"csv_file/off_english.csv\"\n",
    "input_file = \"csv_file/off_rows.csv\"\n",
    "output_file = \"csv_file/off_normalized_by_pipeline.csv\"\n",
    "column_to_normalize = \"product_name\"\n",
    "column_normalized = 'product_name_normalized'\n",
    "pipeline(\n",
    "    input_file=input_file, output_file=output_file, show_something=True, show_all=False, column_name=column_to_normalize, delimiter=\"\\t\", new_column_name=column_normalized\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "from main import read_specified_columns #type: ignore\n",
    "list1 = read_specified_columns(\"csv_file/ingredient_for_coen_normalized_by_pipeline.csv\", [\"normalized_name\"], delimiter = \",\")\n",
    "list2 = read_specified_columns(\"csv_file/ingredient_for_coen_normalized_byhuman.csv\", [\"normalized_name\"], delimiter = \",\")\n",
    "\n",
    "if len(list1) != len(list2):\n",
    "    raise ValueError(\"different length\")\n",
    "\n",
    "kappa_score = cohen_kappa_score(list1, list2)\n",
    "\n",
    "print(f\"Cohen's Kappa Score: {kappa_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambientez",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
