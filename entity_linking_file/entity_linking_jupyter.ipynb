{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMILARITY WITH A CLASSIC BERT (Naive approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entity_linking import find_most_similar_pairs, read_specified_columns\n",
    "\n",
    "file1_path = \"../csv_file/entity_linking_test.csv\"\n",
    "\n",
    "lists = read_specified_columns(file_path=file1_path, elenco_colonne=[\"off\", \"foodkg\"], delimiter=\",\")\n",
    "\n",
    "list1 = [item[0] for item in lists]\n",
    "list2 = [item[1] for item in lists]\n",
    "\n",
    "most_similar_pairs = find_most_similar_pairs(list1, list2)\n",
    "\n",
    "print(\"Most similar couples:\\n\")\n",
    "for item1, item2, score in most_similar_pairs:\n",
    "    print(f\"({item1}) --- ({item2}) --- Similarity: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNIVERSITY OF BARI METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from entity_linking import (\n",
    "    RecipeTransformer,\n",
    "    compute_embeddings,\n",
    "    find_similar_by_title,\n",
    "    read_specified_columns,\n",
    ")\n",
    "\n",
    "file1_path = \"../csv_file/entity_linking_test.csv\"\n",
    "\n",
    "lists = read_specified_columns(file_path=file1_path, elenco_colonne=[\"off\", \"foodkg\"], delimiter=\",\")\n",
    "\n",
    "list1 = [item[0] for item in lists]\n",
    "list2 = [item[1] for item in lists]\n",
    "\n",
    "\n",
    "# Initialize the transformer\n",
    "transformer_name = \"davanstrien/autotrain-recipes-2451975973\"\n",
    "transformer = RecipeTransformer(transformer_name)\n",
    "\n",
    "# Compute embeddings for all recipes in list2\n",
    "print(\"Calculating embeddings for list2...\")\n",
    "embeddings2 = compute_embeddings(list2, transformer)\n",
    "\n",
    "# Create a list of tuples (index, title) for list2\n",
    "entities_list2 = list(enumerate(iterable=list2))\n",
    "\n",
    "# Find the most similar recipe for each item in list1\n",
    "most_similar_pairs = []\n",
    "print(\"Searching for the most similar recipes...\")\n",
    "for recipe_title in tqdm(list1, desc=\"Similarity search\"):\n",
    "    similar_recipe, similarity_score = find_similar_by_title(\n",
    "        recipe_title, entities_list2, embeddings2, transformer\n",
    "    )\n",
    "    most_similar_pairs.append((recipe_title, similar_recipe[1], similarity_score))\n",
    "\n",
    "# Output the results\n",
    "print(\"Most similar recipe pairs found:\\n\")\n",
    "for item1, item2, score in most_similar_pairs:\n",
    "    print(f\"({item1}) --------- ({item2}) --------- Similarity: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBRID METHOD WITH INDICATOR TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entity_linking import find_k_most_similar_pairs_with_indicators\n",
    "\n",
    "list1 = [(\"Pasta\", 30, 5, 10, \"Pasta\"), (\"Pane\", 50, 1, 10, \"Pane\")]\n",
    "list2 = [(\"Riso\", 40, 2, 8, \"Riso\"), (\"Pizza\", 20, 10, 12, \"Pizza\")]\n",
    "result = find_k_most_similar_pairs_with_indicators(list1, list2, use_indicator=True)\n",
    "print(\"actual contribution\", result)\n",
    "\n",
    "list1 = [(\"Pasta\", 100, 0, 0, \"Pasta\"), (\"Pane\", 0, 0, 0, \"Pane\")]\n",
    "list2 = [(\"Riso\", 0, 2, 8, \"Riso\"), (\"Pizza\", 0, 50, 50, \"Pizza\")]\n",
    "result = find_k_most_similar_pairs_with_indicators(list1, list2, use_indicator=True)\n",
    "print(\"negative contribution\", result)\n",
    "\n",
    "list1 = [(\"Pasta\", 33, 33, 33, \"Pasta\"), (\"Pane\", 0, 0, 0, \"Pane\")]\n",
    "list2 = [(\"Riso\", 0, 2, 8, \"Riso\"), (\"Pizza\", 33, 33, 33 , \"Pizza\")]\n",
    "result = find_k_most_similar_pairs_with_indicators(list1, list2, use_indicator=True)\n",
    "print(\"positive contribution\", result)\n",
    "\n",
    "list1 = [(\"pasta\", \"pasta\"), (\"pane\", \"pane\")]\n",
    "list2 = [(\"riso\", \"riso\"), (\"pizza\", \"pizza\")]\n",
    "result = find_k_most_similar_pairs_with_indicators(list1, list2)\n",
    "print(\"no contribution\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST OF VARIOUS BERT MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search of the best threshold value for the bert on a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fa58cc68e6432eac1588bfa4d26bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gzedda/miniconda3/envs/ambientez/lib/python3.13/contextlib.py:109: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60e7665eb9c406186082b2f830d410d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9862afe4b64b04a919200d99950675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c3ffce20614b55ad4d43638b6c20ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f6675827164f719ee906ba2bf994cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a537f8712484520bbb77d36323db6e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gzedda/miniconda3/envs/ambientez/lib/python3.13/site-packages/xformers/__init__.py\", line 57, in _is_triton_available\n",
      "    import triton  # noqa\n",
      "    ^^^^^^^^^^^^^\n",
      "ModuleNotFoundError: No module named 'triton'\n",
      "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No sentence-transformers model found with name infgrad/stella-base-en-v2. Creating a new one with mean pooling.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "No sentence-transformers model found with name BAAI/bge-en-icl. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27bf4068a75b45e4b146484f8b74745c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fbb62b032b0410e870d99465e342513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db255733f10e4ea789000b38dcb13539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf0b1d1735540b18a533072189500e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "Some weights of XLMRobertaLoRA were not initialized from the model checkpoint at ilhamdprastyo/jina-embeddings-v3-tei and are newly initialized: ['roberta.embeddings.position_embeddings.parametrizations.weight.0.lora_A', 'roberta.embeddings.position_embeddings.parametrizations.weight.0.lora_B', 'roberta.embeddings.position_embeddings.parametrizations.weight.original']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ilhamdprastyo/jina-embeddings-v3-tei were not used when initializing XLMRobertaModel: ['roberta.emb_ln.bias', 'roberta.emb_ln.weight', 'roberta.embeddings.token_type_embeddings.parametrizations.weight.0.lora_A', 'roberta.embeddings.token_type_embeddings.parametrizations.weight.0.lora_B', 'roberta.embeddings.token_type_embeddings.parametrizations.weight.original', 'roberta.embeddings.word_embeddings.parametrizations.weight.0.lora_A', 'roberta.embeddings.word_embeddings.parametrizations.weight.0.lora_B', 'roberta.embeddings.word_embeddings.parametrizations.weight.original', 'roberta.encoder.layers.0.mixer.Wqkv.bias', 'roberta.encoder.layers.0.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.0.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.0.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.0.mixer.out_proj.bias', 'roberta.encoder.layers.0.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.0.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.0.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.0.mlp.fc1.bias', 'roberta.encoder.layers.0.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.0.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.0.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.0.mlp.fc2.bias', 'roberta.encoder.layers.0.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.0.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.0.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.0.norm1.bias', 'roberta.encoder.layers.0.norm1.weight', 'roberta.encoder.layers.0.norm2.bias', 'roberta.encoder.layers.0.norm2.weight', 'roberta.encoder.layers.1.mixer.Wqkv.bias', 'roberta.encoder.layers.1.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.1.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.1.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.1.mixer.out_proj.bias', 'roberta.encoder.layers.1.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.1.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.1.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.1.mlp.fc1.bias', 'roberta.encoder.layers.1.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.1.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.1.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.1.mlp.fc2.bias', 'roberta.encoder.layers.1.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.1.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.1.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.1.norm1.bias', 'roberta.encoder.layers.1.norm1.weight', 'roberta.encoder.layers.1.norm2.bias', 'roberta.encoder.layers.1.norm2.weight', 'roberta.encoder.layers.10.mixer.Wqkv.bias', 'roberta.encoder.layers.10.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.10.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.10.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.10.mixer.out_proj.bias', 'roberta.encoder.layers.10.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.10.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.10.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.10.mlp.fc1.bias', 'roberta.encoder.layers.10.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.10.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.10.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.10.mlp.fc2.bias', 'roberta.encoder.layers.10.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.10.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.10.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.10.norm1.bias', 'roberta.encoder.layers.10.norm1.weight', 'roberta.encoder.layers.10.norm2.bias', 'roberta.encoder.layers.10.norm2.weight', 'roberta.encoder.layers.11.mixer.Wqkv.bias', 'roberta.encoder.layers.11.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.11.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.11.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.11.mixer.out_proj.bias', 'roberta.encoder.layers.11.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.11.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.11.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.11.mlp.fc1.bias', 'roberta.encoder.layers.11.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.11.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.11.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.11.mlp.fc2.bias', 'roberta.encoder.layers.11.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.11.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.11.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.11.norm1.bias', 'roberta.encoder.layers.11.norm1.weight', 'roberta.encoder.layers.11.norm2.bias', 'roberta.encoder.layers.11.norm2.weight', 'roberta.encoder.layers.12.mixer.Wqkv.bias', 'roberta.encoder.layers.12.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.12.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.12.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.12.mixer.out_proj.bias', 'roberta.encoder.layers.12.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.12.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.12.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.12.mlp.fc1.bias', 'roberta.encoder.layers.12.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.12.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.12.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.12.mlp.fc2.bias', 'roberta.encoder.layers.12.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.12.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.12.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.12.norm1.bias', 'roberta.encoder.layers.12.norm1.weight', 'roberta.encoder.layers.12.norm2.bias', 'roberta.encoder.layers.12.norm2.weight', 'roberta.encoder.layers.13.mixer.Wqkv.bias', 'roberta.encoder.layers.13.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.13.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.13.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.13.mixer.out_proj.bias', 'roberta.encoder.layers.13.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.13.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.13.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.13.mlp.fc1.bias', 'roberta.encoder.layers.13.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.13.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.13.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.13.mlp.fc2.bias', 'roberta.encoder.layers.13.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.13.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.13.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.13.norm1.bias', 'roberta.encoder.layers.13.norm1.weight', 'roberta.encoder.layers.13.norm2.bias', 'roberta.encoder.layers.13.norm2.weight', 'roberta.encoder.layers.14.mixer.Wqkv.bias', 'roberta.encoder.layers.14.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.14.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.14.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.14.mixer.out_proj.bias', 'roberta.encoder.layers.14.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.14.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.14.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.14.mlp.fc1.bias', 'roberta.encoder.layers.14.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.14.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.14.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.14.mlp.fc2.bias', 'roberta.encoder.layers.14.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.14.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.14.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.14.norm1.bias', 'roberta.encoder.layers.14.norm1.weight', 'roberta.encoder.layers.14.norm2.bias', 'roberta.encoder.layers.14.norm2.weight', 'roberta.encoder.layers.15.mixer.Wqkv.bias', 'roberta.encoder.layers.15.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.15.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.15.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.15.mixer.out_proj.bias', 'roberta.encoder.layers.15.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.15.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.15.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.15.mlp.fc1.bias', 'roberta.encoder.layers.15.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.15.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.15.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.15.mlp.fc2.bias', 'roberta.encoder.layers.15.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.15.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.15.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.15.norm1.bias', 'roberta.encoder.layers.15.norm1.weight', 'roberta.encoder.layers.15.norm2.bias', 'roberta.encoder.layers.15.norm2.weight', 'roberta.encoder.layers.16.mixer.Wqkv.bias', 'roberta.encoder.layers.16.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.16.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.16.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.16.mixer.out_proj.bias', 'roberta.encoder.layers.16.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.16.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.16.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.16.mlp.fc1.bias', 'roberta.encoder.layers.16.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.16.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.16.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.16.mlp.fc2.bias', 'roberta.encoder.layers.16.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.16.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.16.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.16.norm1.bias', 'roberta.encoder.layers.16.norm1.weight', 'roberta.encoder.layers.16.norm2.bias', 'roberta.encoder.layers.16.norm2.weight', 'roberta.encoder.layers.17.mixer.Wqkv.bias', 'roberta.encoder.layers.17.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.17.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.17.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.17.mixer.out_proj.bias', 'roberta.encoder.layers.17.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.17.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.17.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.17.mlp.fc1.bias', 'roberta.encoder.layers.17.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.17.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.17.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.17.mlp.fc2.bias', 'roberta.encoder.layers.17.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.17.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.17.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.17.norm1.bias', 'roberta.encoder.layers.17.norm1.weight', 'roberta.encoder.layers.17.norm2.bias', 'roberta.encoder.layers.17.norm2.weight', 'roberta.encoder.layers.18.mixer.Wqkv.bias', 'roberta.encoder.layers.18.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.18.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.18.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.18.mixer.out_proj.bias', 'roberta.encoder.layers.18.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.18.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.18.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.18.mlp.fc1.bias', 'roberta.encoder.layers.18.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.18.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.18.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.18.mlp.fc2.bias', 'roberta.encoder.layers.18.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.18.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.18.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.18.norm1.bias', 'roberta.encoder.layers.18.norm1.weight', 'roberta.encoder.layers.18.norm2.bias', 'roberta.encoder.layers.18.norm2.weight', 'roberta.encoder.layers.19.mixer.Wqkv.bias', 'roberta.encoder.layers.19.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.19.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.19.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.19.mixer.out_proj.bias', 'roberta.encoder.layers.19.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.19.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.19.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.19.mlp.fc1.bias', 'roberta.encoder.layers.19.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.19.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.19.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.19.mlp.fc2.bias', 'roberta.encoder.layers.19.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.19.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.19.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.19.norm1.bias', 'roberta.encoder.layers.19.norm1.weight', 'roberta.encoder.layers.19.norm2.bias', 'roberta.encoder.layers.19.norm2.weight', 'roberta.encoder.layers.2.mixer.Wqkv.bias', 'roberta.encoder.layers.2.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.2.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.2.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.2.mixer.out_proj.bias', 'roberta.encoder.layers.2.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.2.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.2.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.2.mlp.fc1.bias', 'roberta.encoder.layers.2.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.2.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.2.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.2.mlp.fc2.bias', 'roberta.encoder.layers.2.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.2.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.2.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.2.norm1.bias', 'roberta.encoder.layers.2.norm1.weight', 'roberta.encoder.layers.2.norm2.bias', 'roberta.encoder.layers.2.norm2.weight', 'roberta.encoder.layers.20.mixer.Wqkv.bias', 'roberta.encoder.layers.20.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.20.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.20.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.20.mixer.out_proj.bias', 'roberta.encoder.layers.20.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.20.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.20.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.20.mlp.fc1.bias', 'roberta.encoder.layers.20.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.20.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.20.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.20.mlp.fc2.bias', 'roberta.encoder.layers.20.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.20.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.20.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.20.norm1.bias', 'roberta.encoder.layers.20.norm1.weight', 'roberta.encoder.layers.20.norm2.bias', 'roberta.encoder.layers.20.norm2.weight', 'roberta.encoder.layers.21.mixer.Wqkv.bias', 'roberta.encoder.layers.21.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.21.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.21.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.21.mixer.out_proj.bias', 'roberta.encoder.layers.21.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.21.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.21.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.21.mlp.fc1.bias', 'roberta.encoder.layers.21.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.21.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.21.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.21.mlp.fc2.bias', 'roberta.encoder.layers.21.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.21.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.21.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.21.norm1.bias', 'roberta.encoder.layers.21.norm1.weight', 'roberta.encoder.layers.21.norm2.bias', 'roberta.encoder.layers.21.norm2.weight', 'roberta.encoder.layers.22.mixer.Wqkv.bias', 'roberta.encoder.layers.22.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.22.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.22.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.22.mixer.out_proj.bias', 'roberta.encoder.layers.22.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.22.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.22.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.22.mlp.fc1.bias', 'roberta.encoder.layers.22.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.22.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.22.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.22.mlp.fc2.bias', 'roberta.encoder.layers.22.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.22.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.22.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.22.norm1.bias', 'roberta.encoder.layers.22.norm1.weight', 'roberta.encoder.layers.22.norm2.bias', 'roberta.encoder.layers.22.norm2.weight', 'roberta.encoder.layers.23.mixer.Wqkv.bias', 'roberta.encoder.layers.23.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.23.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.23.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.23.mixer.out_proj.bias', 'roberta.encoder.layers.23.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.23.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.23.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.23.mlp.fc1.bias', 'roberta.encoder.layers.23.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.23.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.23.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.23.mlp.fc2.bias', 'roberta.encoder.layers.23.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.23.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.23.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.23.norm1.bias', 'roberta.encoder.layers.23.norm1.weight', 'roberta.encoder.layers.23.norm2.bias', 'roberta.encoder.layers.23.norm2.weight', 'roberta.encoder.layers.3.mixer.Wqkv.bias', 'roberta.encoder.layers.3.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.3.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.3.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.3.mixer.out_proj.bias', 'roberta.encoder.layers.3.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.3.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.3.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.3.mlp.fc1.bias', 'roberta.encoder.layers.3.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.3.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.3.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.3.mlp.fc2.bias', 'roberta.encoder.layers.3.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.3.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.3.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.3.norm1.bias', 'roberta.encoder.layers.3.norm1.weight', 'roberta.encoder.layers.3.norm2.bias', 'roberta.encoder.layers.3.norm2.weight', 'roberta.encoder.layers.4.mixer.Wqkv.bias', 'roberta.encoder.layers.4.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.4.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.4.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.4.mixer.out_proj.bias', 'roberta.encoder.layers.4.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.4.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.4.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.4.mlp.fc1.bias', 'roberta.encoder.layers.4.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.4.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.4.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.4.mlp.fc2.bias', 'roberta.encoder.layers.4.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.4.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.4.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.4.norm1.bias', 'roberta.encoder.layers.4.norm1.weight', 'roberta.encoder.layers.4.norm2.bias', 'roberta.encoder.layers.4.norm2.weight', 'roberta.encoder.layers.5.mixer.Wqkv.bias', 'roberta.encoder.layers.5.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.5.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.5.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.5.mixer.out_proj.bias', 'roberta.encoder.layers.5.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.5.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.5.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.5.mlp.fc1.bias', 'roberta.encoder.layers.5.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.5.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.5.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.5.mlp.fc2.bias', 'roberta.encoder.layers.5.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.5.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.5.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.5.norm1.bias', 'roberta.encoder.layers.5.norm1.weight', 'roberta.encoder.layers.5.norm2.bias', 'roberta.encoder.layers.5.norm2.weight', 'roberta.encoder.layers.6.mixer.Wqkv.bias', 'roberta.encoder.layers.6.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.6.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.6.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.6.mixer.out_proj.bias', 'roberta.encoder.layers.6.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.6.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.6.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.6.mlp.fc1.bias', 'roberta.encoder.layers.6.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.6.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.6.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.6.mlp.fc2.bias', 'roberta.encoder.layers.6.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.6.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.6.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.6.norm1.bias', 'roberta.encoder.layers.6.norm1.weight', 'roberta.encoder.layers.6.norm2.bias', 'roberta.encoder.layers.6.norm2.weight', 'roberta.encoder.layers.7.mixer.Wqkv.bias', 'roberta.encoder.layers.7.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.7.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.7.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.7.mixer.out_proj.bias', 'roberta.encoder.layers.7.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.7.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.7.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.7.mlp.fc1.bias', 'roberta.encoder.layers.7.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.7.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.7.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.7.mlp.fc2.bias', 'roberta.encoder.layers.7.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.7.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.7.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.7.norm1.bias', 'roberta.encoder.layers.7.norm1.weight', 'roberta.encoder.layers.7.norm2.bias', 'roberta.encoder.layers.7.norm2.weight', 'roberta.encoder.layers.8.mixer.Wqkv.bias', 'roberta.encoder.layers.8.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.8.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.8.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.8.mixer.out_proj.bias', 'roberta.encoder.layers.8.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.8.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.8.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.8.mlp.fc1.bias', 'roberta.encoder.layers.8.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.8.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.8.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.8.mlp.fc2.bias', 'roberta.encoder.layers.8.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.8.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.8.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.8.norm1.bias', 'roberta.encoder.layers.8.norm1.weight', 'roberta.encoder.layers.8.norm2.bias', 'roberta.encoder.layers.8.norm2.weight', 'roberta.encoder.layers.9.mixer.Wqkv.bias', 'roberta.encoder.layers.9.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.9.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.9.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.9.mixer.out_proj.bias', 'roberta.encoder.layers.9.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.9.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.9.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.9.mlp.fc1.bias', 'roberta.encoder.layers.9.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.9.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.9.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.9.mlp.fc2.bias', 'roberta.encoder.layers.9.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.9.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.9.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.9.norm1.bias', 'roberta.encoder.layers.9.norm1.weight', 'roberta.encoder.layers.9.norm2.bias', 'roberta.encoder.layers.9.norm2.weight', 'roberta.pooler.dense.parametrizations.weight.0.lora_A', 'roberta.pooler.dense.parametrizations.weight.0.lora_B', 'roberta.pooler.dense.parametrizations.weight.original']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at ilhamdprastyo/jina-embeddings-v3-tei and are newly initialized: ['roberta.embeddings.LayerNorm.bias', 'roberta.embeddings.LayerNorm.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.12.attention.output.LayerNorm.bias', 'roberta.encoder.layer.12.attention.output.LayerNorm.weight', 'roberta.encoder.layer.12.attention.output.dense.bias', 'roberta.encoder.layer.12.attention.output.dense.weight', 'roberta.encoder.layer.12.attention.self.key.bias', 'roberta.encoder.layer.12.attention.self.key.weight', 'roberta.encoder.layer.12.attention.self.query.bias', 'roberta.encoder.layer.12.attention.self.query.weight', 'roberta.encoder.layer.12.attention.self.value.bias', 'roberta.encoder.layer.12.attention.self.value.weight', 'roberta.encoder.layer.12.intermediate.dense.bias', 'roberta.encoder.layer.12.intermediate.dense.weight', 'roberta.encoder.layer.12.output.LayerNorm.bias', 'roberta.encoder.layer.12.output.LayerNorm.weight', 'roberta.encoder.layer.12.output.dense.bias', 'roberta.encoder.layer.12.output.dense.weight', 'roberta.encoder.layer.13.attention.output.LayerNorm.bias', 'roberta.encoder.layer.13.attention.output.LayerNorm.weight', 'roberta.encoder.layer.13.attention.output.dense.bias', 'roberta.encoder.layer.13.attention.output.dense.weight', 'roberta.encoder.layer.13.attention.self.key.bias', 'roberta.encoder.layer.13.attention.self.key.weight', 'roberta.encoder.layer.13.attention.self.query.bias', 'roberta.encoder.layer.13.attention.self.query.weight', 'roberta.encoder.layer.13.attention.self.value.bias', 'roberta.encoder.layer.13.attention.self.value.weight', 'roberta.encoder.layer.13.intermediate.dense.bias', 'roberta.encoder.layer.13.intermediate.dense.weight', 'roberta.encoder.layer.13.output.LayerNorm.bias', 'roberta.encoder.layer.13.output.LayerNorm.weight', 'roberta.encoder.layer.13.output.dense.bias', 'roberta.encoder.layer.13.output.dense.weight', 'roberta.encoder.layer.14.attention.output.LayerNorm.bias', 'roberta.encoder.layer.14.attention.output.LayerNorm.weight', 'roberta.encoder.layer.14.attention.output.dense.bias', 'roberta.encoder.layer.14.attention.output.dense.weight', 'roberta.encoder.layer.14.attention.self.key.bias', 'roberta.encoder.layer.14.attention.self.key.weight', 'roberta.encoder.layer.14.attention.self.query.bias', 'roberta.encoder.layer.14.attention.self.query.weight', 'roberta.encoder.layer.14.attention.self.value.bias', 'roberta.encoder.layer.14.attention.self.value.weight', 'roberta.encoder.layer.14.intermediate.dense.bias', 'roberta.encoder.layer.14.intermediate.dense.weight', 'roberta.encoder.layer.14.output.LayerNorm.bias', 'roberta.encoder.layer.14.output.LayerNorm.weight', 'roberta.encoder.layer.14.output.dense.bias', 'roberta.encoder.layer.14.output.dense.weight', 'roberta.encoder.layer.15.attention.output.LayerNorm.bias', 'roberta.encoder.layer.15.attention.output.LayerNorm.weight', 'roberta.encoder.layer.15.attention.output.dense.bias', 'roberta.encoder.layer.15.attention.output.dense.weight', 'roberta.encoder.layer.15.attention.self.key.bias', 'roberta.encoder.layer.15.attention.self.key.weight', 'roberta.encoder.layer.15.attention.self.query.bias', 'roberta.encoder.layer.15.attention.self.query.weight', 'roberta.encoder.layer.15.attention.self.value.bias', 'roberta.encoder.layer.15.attention.self.value.weight', 'roberta.encoder.layer.15.intermediate.dense.bias', 'roberta.encoder.layer.15.intermediate.dense.weight', 'roberta.encoder.layer.15.output.LayerNorm.bias', 'roberta.encoder.layer.15.output.LayerNorm.weight', 'roberta.encoder.layer.15.output.dense.bias', 'roberta.encoder.layer.15.output.dense.weight', 'roberta.encoder.layer.16.attention.output.LayerNorm.bias', 'roberta.encoder.layer.16.attention.output.LayerNorm.weight', 'roberta.encoder.layer.16.attention.output.dense.bias', 'roberta.encoder.layer.16.attention.output.dense.weight', 'roberta.encoder.layer.16.attention.self.key.bias', 'roberta.encoder.layer.16.attention.self.key.weight', 'roberta.encoder.layer.16.attention.self.query.bias', 'roberta.encoder.layer.16.attention.self.query.weight', 'roberta.encoder.layer.16.attention.self.value.bias', 'roberta.encoder.layer.16.attention.self.value.weight', 'roberta.encoder.layer.16.intermediate.dense.bias', 'roberta.encoder.layer.16.intermediate.dense.weight', 'roberta.encoder.layer.16.output.LayerNorm.bias', 'roberta.encoder.layer.16.output.LayerNorm.weight', 'roberta.encoder.layer.16.output.dense.bias', 'roberta.encoder.layer.16.output.dense.weight', 'roberta.encoder.layer.17.attention.output.LayerNorm.bias', 'roberta.encoder.layer.17.attention.output.LayerNorm.weight', 'roberta.encoder.layer.17.attention.output.dense.bias', 'roberta.encoder.layer.17.attention.output.dense.weight', 'roberta.encoder.layer.17.attention.self.key.bias', 'roberta.encoder.layer.17.attention.self.key.weight', 'roberta.encoder.layer.17.attention.self.query.bias', 'roberta.encoder.layer.17.attention.self.query.weight', 'roberta.encoder.layer.17.attention.self.value.bias', 'roberta.encoder.layer.17.attention.self.value.weight', 'roberta.encoder.layer.17.intermediate.dense.bias', 'roberta.encoder.layer.17.intermediate.dense.weight', 'roberta.encoder.layer.17.output.LayerNorm.bias', 'roberta.encoder.layer.17.output.LayerNorm.weight', 'roberta.encoder.layer.17.output.dense.bias', 'roberta.encoder.layer.17.output.dense.weight', 'roberta.encoder.layer.18.attention.output.LayerNorm.bias', 'roberta.encoder.layer.18.attention.output.LayerNorm.weight', 'roberta.encoder.layer.18.attention.output.dense.bias', 'roberta.encoder.layer.18.attention.output.dense.weight', 'roberta.encoder.layer.18.attention.self.key.bias', 'roberta.encoder.layer.18.attention.self.key.weight', 'roberta.encoder.layer.18.attention.self.query.bias', 'roberta.encoder.layer.18.attention.self.query.weight', 'roberta.encoder.layer.18.attention.self.value.bias', 'roberta.encoder.layer.18.attention.self.value.weight', 'roberta.encoder.layer.18.intermediate.dense.bias', 'roberta.encoder.layer.18.intermediate.dense.weight', 'roberta.encoder.layer.18.output.LayerNorm.bias', 'roberta.encoder.layer.18.output.LayerNorm.weight', 'roberta.encoder.layer.18.output.dense.bias', 'roberta.encoder.layer.18.output.dense.weight', 'roberta.encoder.layer.19.attention.output.LayerNorm.bias', 'roberta.encoder.layer.19.attention.output.LayerNorm.weight', 'roberta.encoder.layer.19.attention.output.dense.bias', 'roberta.encoder.layer.19.attention.output.dense.weight', 'roberta.encoder.layer.19.attention.self.key.bias', 'roberta.encoder.layer.19.attention.self.key.weight', 'roberta.encoder.layer.19.attention.self.query.bias', 'roberta.encoder.layer.19.attention.self.query.weight', 'roberta.encoder.layer.19.attention.self.value.bias', 'roberta.encoder.layer.19.attention.self.value.weight', 'roberta.encoder.layer.19.intermediate.dense.bias', 'roberta.encoder.layer.19.intermediate.dense.weight', 'roberta.encoder.layer.19.output.LayerNorm.bias', 'roberta.encoder.layer.19.output.LayerNorm.weight', 'roberta.encoder.layer.19.output.dense.bias', 'roberta.encoder.layer.19.output.dense.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.20.attention.output.LayerNorm.bias', 'roberta.encoder.layer.20.attention.output.LayerNorm.weight', 'roberta.encoder.layer.20.attention.output.dense.bias', 'roberta.encoder.layer.20.attention.output.dense.weight', 'roberta.encoder.layer.20.attention.self.key.bias', 'roberta.encoder.layer.20.attention.self.key.weight', 'roberta.encoder.layer.20.attention.self.query.bias', 'roberta.encoder.layer.20.attention.self.query.weight', 'roberta.encoder.layer.20.attention.self.value.bias', 'roberta.encoder.layer.20.attention.self.value.weight', 'roberta.encoder.layer.20.intermediate.dense.bias', 'roberta.encoder.layer.20.intermediate.dense.weight', 'roberta.encoder.layer.20.output.LayerNorm.bias', 'roberta.encoder.layer.20.output.LayerNorm.weight', 'roberta.encoder.layer.20.output.dense.bias', 'roberta.encoder.layer.20.output.dense.weight', 'roberta.encoder.layer.21.attention.output.LayerNorm.bias', 'roberta.encoder.layer.21.attention.output.LayerNorm.weight', 'roberta.encoder.layer.21.attention.output.dense.bias', 'roberta.encoder.layer.21.attention.output.dense.weight', 'roberta.encoder.layer.21.attention.self.key.bias', 'roberta.encoder.layer.21.attention.self.key.weight', 'roberta.encoder.layer.21.attention.self.query.bias', 'roberta.encoder.layer.21.attention.self.query.weight', 'roberta.encoder.layer.21.attention.self.value.bias', 'roberta.encoder.layer.21.attention.self.value.weight', 'roberta.encoder.layer.21.intermediate.dense.bias', 'roberta.encoder.layer.21.intermediate.dense.weight', 'roberta.encoder.layer.21.output.LayerNorm.bias', 'roberta.encoder.layer.21.output.LayerNorm.weight', 'roberta.encoder.layer.21.output.dense.bias', 'roberta.encoder.layer.21.output.dense.weight', 'roberta.encoder.layer.22.attention.output.LayerNorm.bias', 'roberta.encoder.layer.22.attention.output.LayerNorm.weight', 'roberta.encoder.layer.22.attention.output.dense.bias', 'roberta.encoder.layer.22.attention.output.dense.weight', 'roberta.encoder.layer.22.attention.self.key.bias', 'roberta.encoder.layer.22.attention.self.key.weight', 'roberta.encoder.layer.22.attention.self.query.bias', 'roberta.encoder.layer.22.attention.self.query.weight', 'roberta.encoder.layer.22.attention.self.value.bias', 'roberta.encoder.layer.22.attention.self.value.weight', 'roberta.encoder.layer.22.intermediate.dense.bias', 'roberta.encoder.layer.22.intermediate.dense.weight', 'roberta.encoder.layer.22.output.LayerNorm.bias', 'roberta.encoder.layer.22.output.LayerNorm.weight', 'roberta.encoder.layer.22.output.dense.bias', 'roberta.encoder.layer.22.output.dense.weight', 'roberta.encoder.layer.23.attention.output.LayerNorm.bias', 'roberta.encoder.layer.23.attention.output.LayerNorm.weight', 'roberta.encoder.layer.23.attention.output.dense.bias', 'roberta.encoder.layer.23.attention.output.dense.weight', 'roberta.encoder.layer.23.attention.self.key.bias', 'roberta.encoder.layer.23.attention.self.key.weight', 'roberta.encoder.layer.23.attention.self.query.bias', 'roberta.encoder.layer.23.attention.self.query.weight', 'roberta.encoder.layer.23.attention.self.value.bias', 'roberta.encoder.layer.23.attention.self.value.weight', 'roberta.encoder.layer.23.intermediate.dense.bias', 'roberta.encoder.layer.23.intermediate.dense.weight', 'roberta.encoder.layer.23.output.LayerNorm.bias', 'roberta.encoder.layer.23.output.LayerNorm.weight', 'roberta.encoder.layer.23.output.dense.bias', 'roberta.encoder.layer.23.output.dense.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file created ../csv_file/bert_comparison_validation.csv.\n"
     ]
    }
   ],
   "source": [
    "from entity_linking import read_specified_columns, evaluate_entity_linking_method\n",
    "import csv\n",
    "\n",
    "file_path = \"../csv_file/entity_linking_test_normalized_validation.csv\"\n",
    "column_list = [\"off_normalized\", \"foodkg_normalized\"]\n",
    "data = read_specified_columns(file_path, elenco_colonne=column_list, delimiter=\",\")\n",
    "\n",
    "#https://huggingface.co/spaces/mteb/leaderboard 09/12/2024\n",
    "list_of_models = [\n",
    "    \n",
    "    #top 5 in pair classification (around 10000000 parameter)\n",
    "    #voyage is not free\n",
    "    #\"meta-llama/Meta-Llama-3-8B-Instruct\", # have problem with the token's padding\n",
    "    \"nvidia/NV-Embed-v2\",\n",
    "    \"Salesforce/SFR-Embedding-Mistral\",\n",
    "    \"compressa-ai/Compressa-Embeddings\",\n",
    "\n",
    "    # top 3 under 1000000 parameters\n",
    "    \"dunzhang/stella_en_400M_v5\",\n",
    "    \"llmrails/ember-v1\",\n",
    "    \"WhereIsAI/UAE-Large-V1\",\n",
    "\n",
    "    # top 3 under 100000 parameters\n",
    "    \"infgrad/stella-base-en-v2\",\n",
    "    \"intfloat/e5-small\",\n",
    "    \"BAAI/bge-small-en-v1.5\", \n",
    "\n",
    "    #top 5 overall\n",
    "    #nvidia/NV-Embed-v2 alredy tested\n",
    "    \"dunzhang/stella_en_1.5B_v5\",\n",
    "    \"BAAI/bge-en-icl\",\n",
    "    \"blevlabs/stella_en_v5\",\n",
    "    \"Salesforce/SFR-Embedding-2_R\",\n",
    "\n",
    "    # top 3 in sts\n",
    "    \"Lajavaness/bilingual-embedding-large\",\n",
    "    \"ilhamdprastyo/jina-embeddings-v3-tei\",\n",
    "    \"jinaai/jina-embeddings-v3\"\n",
    "    ]\n",
    "\n",
    "\n",
    "column_names = [\"model_name\", \"vocab_size\", \"number_of_parameters\", \"accuracy\", \"accuracy_on_considered\", \"number_of_TP_and_TN\", \"threshold\", ]\n",
    "threshold = [(i/100) for i in  range(80, 100, 1)]\n",
    "\n",
    "output_file = \"../csv_file/bert_comparison_validation.csv\"\n",
    "\n",
    "with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(column_names)\n",
    "    \n",
    "    for model in list_of_models:\n",
    "        model_name, vocab_size, number_of_parameters, accuracy, accuracy_considered, number_of_TP_and_TN, threshold = evaluate_entity_linking_method(\n",
    "            data, show_progress=False, model=model, threshold_list=threshold\n",
    "        )\n",
    "        for modelz, vocab_sizez, number_of_parametersz, accuracyz, accuracy_consideredz, number_of_TP_and_TNz, thresholdz,  in zip(model_name, vocab_size, number_of_parameters, accuracy, accuracy_considered, number_of_TP_and_TN, threshold):\n",
    "            writer.writerow([model, vocab_sizez, number_of_parametersz, round(accuracyz, 2), round(accuracy_consideredz) , number_of_TP_and_TNz, thresholdz])\n",
    "\n",
    "print(f\"file created {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "determine the best bert on a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nvidia/NV-Embed-v2': ['0.86', '100'], 'Salesforce/SFR-Embedding-Mistral': ['0.97', '100'], 'compressa-ai/Compressa-Embeddings': ['0.97', '100'], 'dunzhang/stella_en_400M_v5': ['0.97', '100'], 'llmrails/ember-v1': ['0.95', '100'], 'WhereIsAI/UAE-Large-V1': ['0.94', '100'], 'infgrad/stella-base-en-v2': ['0.97', '100'], 'intfloat/e5-small': ['0.99', '100'], 'BAAI/bge-small-en-v1.5': ['0.96', '100'], 'dunzhang/stella_en_1.5B_v5': ['0.97', '100'], 'BAAI/bge-en-icl': ['0.85', '100'], 'blevlabs/stella_en_v5': ['0.97', '100'], 'Salesforce/SFR-Embedding-2_R': ['0.97', '100'], 'Lajavaness/bilingual-embedding-large': ['0.88', '100'], 'ilhamdprastyo/jina-embeddings-v3-tei': ['0.8', '0'], 'jinaai/jina-embeddings-v3': ['0.95', '100']}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f34c32174fd4ef2be9fa274eba5f6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gzedda/miniconda3/envs/ambientez/lib/python3.13/contextlib.py:109: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d979bbfe2774e1a9db3b353915c44d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23248fc702074f619ab6fc34a30f1b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ff00f959184acd9bbd0ec19f0ce75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1002f59e3c294c72826a9256172a9a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530acafa2dd24675828190ea2e6d3f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gzedda/miniconda3/envs/ambientez/lib/python3.13/site-packages/xformers/__init__.py\", line 57, in _is_triton_available\n",
      "    import triton  # noqa\n",
      "    ^^^^^^^^^^^^^\n",
      "ModuleNotFoundError: No module named 'triton'\n",
      "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No sentence-transformers model found with name infgrad/stella-base-en-v2. Creating a new one with mean pooling.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "No sentence-transformers model found with name BAAI/bge-en-icl. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7fe870bd2a4fe690b656f0ad1361f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ed3fcb4b454757a1c30f65cb44e71c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a82e3ee2764f69a662cbf14cae7311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30921ab509b54cafbd697b783de86145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "Some weights of XLMRobertaLoRA were not initialized from the model checkpoint at ilhamdprastyo/jina-embeddings-v3-tei and are newly initialized: ['roberta.embeddings.position_embeddings.parametrizations.weight.0.lora_A', 'roberta.embeddings.position_embeddings.parametrizations.weight.0.lora_B', 'roberta.embeddings.position_embeddings.parametrizations.weight.original']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "Some weights of XLMRobertaLoRA were not initialized from the model checkpoint at ilhamdprastyo/jina-embeddings-v3-tei and are newly initialized: ['roberta.embeddings.position_embeddings.parametrizations.weight.0.lora_A', 'roberta.embeddings.position_embeddings.parametrizations.weight.0.lora_B', 'roberta.embeddings.position_embeddings.parametrizations.weight.original']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file created ../csv_file/bert_comparison_on_test_set.csv.\n"
     ]
    }
   ],
   "source": [
    "from entity_linking import read_specified_columns, evaluate_entity_linking_method\n",
    "import csv\n",
    "\n",
    "file_path = \"../csv_file/entity_linking_test_normalized_test.csv\"\n",
    "column_list = [\"off_normalized\", \"foodkg_normalized\"]\n",
    "data = read_specified_columns(file_path, elenco_colonne=column_list, delimiter=\",\")\n",
    "\n",
    "file1_path = \"../csv_file/bert_comparison_validation.csv\"\n",
    "column_list = [\"model_name\", \"threshold\", \"accuracy_on_considered\"]\n",
    "reader = read_specified_columns(file1_path, elenco_colonne=column_list, delimiter=\",\")\n",
    "\n",
    "list_of_models = []\n",
    "list_of_threshold = []\n",
    "list_of_accuracy = []\n",
    "\n",
    "for model, threshold, accuracy in reader:\n",
    "    list_of_models.append(model)\n",
    "    list_of_threshold.append(threshold)\n",
    "    list_of_accuracy.append(accuracy)\n",
    "\n",
    "model_threshold_dictionary = {}\n",
    "\n",
    "for model, threshold, accuracy in zip(list_of_models, list_of_threshold, list_of_accuracy):\n",
    "    if model not in model_threshold_dictionary:\n",
    "        model_threshold_dictionary[model] = [threshold, accuracy]\n",
    "    else:\n",
    "        if ((float(accuracy) > float(model_threshold_dictionary[model][1])) | ((float(accuracy) == float(model_threshold_dictionary[model][1])) & (float(threshold) < float(model_threshold_dictionary[model][0])))):\n",
    "            model_threshold_dictionary[model] = [threshold, accuracy]\n",
    "\n",
    "print(model_threshold_dictionary)\n",
    "\n",
    "\n",
    "\n",
    "column_names = [\"model_name\", \"vocab_size\", \"number_of_parameters\", \"accuracy\", \"accuracy_on_considered\", \"number_of_TP_and_TN\", \"threshold\", ]\n",
    "\n",
    "output_file = \"../csv_file/bert_comparison_on_test_set.csv\"\n",
    "\n",
    "with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(column_names)\n",
    "    for model in model_threshold_dictionary.keys():\n",
    "        threshold = model_threshold_dictionary[model][0]\n",
    "        try:\n",
    "            threshold_list = [float(threshold)]\n",
    "        except ValueError:\n",
    "            print(f\"Invalid threshold for model {model}: {threshold}\")\n",
    "            continue  # Salta il modello se il threshold è invalido\n",
    "\n",
    "        model_name, vocab_size, number_of_parameters, accuracy, accuracy_considered, number_of_TP_and_TN, threshold = evaluate_entity_linking_method(\n",
    "            data, show_progress=False, model=model, threshold_list=threshold_list\n",
    "        )\n",
    "        for modelz, vocab_sizez, number_of_parametersz, accuracyz, accuracy_consideredz, number_of_TP_and_TNz, thresholdz in zip(\n",
    "            model_name, vocab_size, number_of_parameters, accuracy, accuracy_considered, number_of_TP_and_TN, threshold\n",
    "        ):\n",
    "            writer.writerow([modelz, vocab_sizez, number_of_parametersz, round(accuracyz, 2), round(accuracy_consideredz), number_of_TP_and_TNz, thresholdz])\n",
    "\n",
    "print(f\"file created {output_file}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambientez",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
