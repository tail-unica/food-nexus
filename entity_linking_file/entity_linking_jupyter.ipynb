{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMILARITY WITH A CLASSIC BERT (Naive approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f442c57bf5d45078afeca28e2cece0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165162c96fbd49af9f687917987b70a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eff0e2a82e246d1864fdfd5bf639968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141d8cc70074484ab10d6a1c61c0bc9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8a14885e6e4fadbb89e3554a8d3543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830f8974103b4014bf701d80bf5941c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/69.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd60ad16f604ddc89c12f7ef6d40b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b54022b80d44196adec54c2639fb397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b93bf41b534866bd4ca78fd41cde38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5da40aa272344c7b8ed3551892df693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9650b06cfda643f8ad2b5d1c14af60c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar couples:\n",
      "\n",
      "(Protein brownie mini) --- (brownie) --- Similarity: 0.686\n",
      "(Tablet) --- (rennet tablet) --- Similarity: 0.607\n",
      "(Saucisse a tartiner 170g) --- (hake fillets) --- Similarity: 0.327\n",
      "(Noisettes crues) --- (italian - flavored croutons) --- Similarity: 0.411\n",
      "(Hummus gegrilde groenten) --- (hummus) --- Similarity: 0.606\n",
      "(Land O' Frost Premium Cured Roast Beef) --- (roast beef) --- Similarity: 0.743\n",
      "(pepper jelly) --- (pepper jelly) --- Similarity: 1.000\n",
      "(Whole Chocolate Milk) --- (chocolate milk) --- Similarity: 0.951\n",
      "(Filet de poulet) --- (boneless pork filet) --- Similarity: 0.344\n",
      "(Olivenmix mit Kräuter) --- (olive) --- Similarity: 0.621\n",
      "(Rinder-Hackfleisch) --- (white wine vinegar) --- Similarity: 0.247\n",
      "(ICA i♥eco 12 ekologiska ägg från frigående) --- (red enchilada sauce) --- Similarity: 0.272\n",
      "(Harina de trigo especial reposteria) --- (italian - flavored croutons) --- Similarity: 0.281\n",
      "(Sardines in water) --- (sardines) --- Similarity: 0.873\n",
      "(Oeufs frais) --- (oatmeal) --- Similarity: 0.298\n",
      "(Gelée de piments d’espele) --- (hake fillets) --- Similarity: 0.351\n",
      "(Brente Mandler) --- (hake fillets) --- Similarity: 0.262\n",
      "(Cereal) --- (cereal) --- Similarity: 1.000\n",
      "(5 fűszeres paradicsomszó) --- (tzatziki) --- Similarity: 0.307\n",
      "(Jambon traditionnel) --- (red enchilada sauce) --- Similarity: 0.313\n",
      "(Hershey’s cookies ´n’ ) --- (cookies) --- Similarity: 0.702\n",
      "(Salsiccia luganega) --- (salami) --- Similarity: 0.384\n",
      "(Ceviche tilapia) --- (tilapia fillet) --- Similarity: 0.697\n",
      "(Olive oil) --- (olive oil) --- Similarity: 1.000\n",
      "(Fraises) --- (red enchilada sauce) --- Similarity: 0.243\n",
      "(Rillette de Bar Poivre rouge de Kampot) --- (chocolate bar) --- Similarity: 0.311\n",
      "(Export Sodas) --- (7 - Up soda) --- Similarity: 0.598\n",
      "(Vitalis light croccante chioccolato) --- (italian - flavored croutons) --- Similarity: 0.342\n",
      "(Pizza forno de pedra 4 estaçõ) --- (frozen pizza) --- Similarity: 0.470\n",
      "(Nougat Limoncello) --- (limoncello) --- Similarity: 0.768\n",
      "(Glace noisette) --- (tzatziki) --- Similarity: 0.286\n",
      "(Suppennudeln OFFEN) --- (prune) --- Similarity: 0.367\n",
      "(Organic lemons) --- (lemons) --- Similarity: 0.879\n",
      "(Speck alto adige Igp) --- (speck ) --- Similarity: 0.480\n",
      "(Melange d olives) --- (olive) --- Similarity: 0.721\n",
      "(Bens original riz basmati) --- (basmati rice) --- Similarity: 0.372\n",
      "(Emprésuré chocolat inten) --- (hake fillets) --- Similarity: 0.351\n",
      "(Provolone dolce) --- (provolone cheese) --- Similarity: 0.721\n",
      "(Gluten free italian crostini) --- (italian - flavored croutons) --- Similarity: 0.541\n",
      "(Fave Intere Salate) --- (salami) --- Similarity: 0.509\n",
      "(Slightly Salted Butter) --- (lightly - salted butter) --- Similarity: 0.927\n",
      "(Iogurte de Açaí com Banana - Frutap - Frut) --- (banana yogurt) --- Similarity: 0.719\n",
      "(Labeyrie tranches de saumon fum�) --- (raw cacao powder) --- Similarity: 0.341\n",
      "(Sensational Burger) --- (hamburger) --- Similarity: 0.700\n",
      "(Habanero Lime Cream Cheese Wontons) --- (raclette cheese) --- Similarity: 0.555\n",
      "(Smothie) --- (brownie) --- Similarity: 0.428\n",
      "(Saucisse bouillie) --- (roast beef) --- Similarity: 0.300\n",
      "(Sweet & tangy original barbecue sauce) --- (sweet and spicy barbecue sauce) --- Similarity: 0.826\n",
      "(Les grillades du chef brochetyes de poulet chorizo et colombo) --- (sweet and spicy barbecue sauce) --- Similarity: 0.417\n",
      "(Melocotón en Almiba) --- (soymilk) --- Similarity: 0.392\n",
      "(Break oatmeal) --- (oatmeal) --- Similarity: 0.840\n",
      "(Poudre de cacao) --- (raw cacao powder) --- Similarity: 0.641\n",
      "(Penne quattro fromaggi) --- (penne pasta) --- Similarity: 0.417\n",
      "(Moutarde de Dijon) --- (Dijon mustard) --- Similarity: 0.517\n",
      "(Vanille) --- (limoncello) --- Similarity: 0.234\n",
      "(Cocktail d'huiles vierges biologiques Oméga 3&) --- (lemons) --- Similarity: 0.333\n",
      "(Roja Enchilada Sauce) --- (red enchilada sauce) --- Similarity: 0.760\n",
      "(HEB Whole Wheat Ultra Thin & Crispy Pizza Crusts) --- (whole wheat pizza crusts ) --- Similarity: 0.769\n",
      "(pain de mie tranché aux graines de lin et tournesol et à la farine de seig) --- (olive oil) --- Similarity: 0.285\n",
      "(Ciruela pasa) --- (Italian olives) --- Similarity: 0.275\n",
      "(Vloeibare bloemenhoning) --- (wontons) --- Similarity: 0.222\n",
      "(Carotte rapée) --- (limoncello) --- Similarity: 0.365\n",
      "(Bio Café Organic) --- (coffee) --- Similarity: 0.433\n",
      "(Hamburger bovino) --- (hamburger) --- Similarity: 0.771\n",
      "(Merluza Filetes sin piel) --- (whole wheat pizza crusts ) --- Similarity: 0.348\n",
      "(4 vol-au-vent feuilleté) --- (vol - au - vent cases) --- Similarity: 0.624\n",
      "(Iced fancy decorated butter cookies) --- (butter cookies) --- Similarity: 0.794\n",
      "(Purée Pomme & Crème De Marron 100% Plaisir B) --- (italian - flavored croutons) --- Similarity: 0.355\n",
      "(White wine vinegar) --- (white wine vinegar) --- Similarity: 1.000\n",
      "(Sucette en chocolat) --- (pastry flour) --- Similarity: 0.398\n",
      "(Raclette) --- (raclette cheese) --- Similarity: 0.732\n",
      "(Hamburguesa 3) --- (hamburger) --- Similarity: 0.333\n",
      "(Reispapier) --- (tilapia fillet) --- Similarity: 0.368\n",
      "(Haricots blancs) --- (lemons) --- Similarity: 0.420\n",
      "(Classic blend 40 Tea Bags) --- (tea bags) --- Similarity: 0.697\n",
      "(Rodaja de marrajo) --- (limoncello) --- Similarity: 0.257\n",
      "(Corn Flakes) --- (corn flakes) --- Similarity: 1.000\n",
      "(Cocoa puffs cuckoo for chocolatey milk! frosted corn puffs) --- (frozen cream puffs) --- Similarity: 0.624\n",
      "(Sardellenfilets) --- (sardine fillet) --- Similarity: 0.506\n",
      "(Lait demi écré) --- (salami) --- Similarity: 0.220\n",
      "(Pina colada) --- (frozen Pina Colada mix) --- Similarity: 0.783\n",
      "(Croissant salati al sesamo) --- (plain croissants) --- Similarity: 0.660\n",
      "(Rillons de porc) --- (raw cacao powder) --- Similarity: 0.287\n",
      "(Italian Nocellara Olives) --- (Italian olives) --- Similarity: 0.812\n",
      "(Galletas bio) --- (Italian olives) --- Similarity: 0.255\n",
      "(Carolina Gold Sugar Free BBQ Sauce) --- (sweet and spicy barbecue sauce) --- Similarity: 0.569\n",
      "(Espresso cremoso compatibile Dolce Gusto) --- (espresso coffee ) --- Similarity: 0.442\n",
      "(Pilons de poulet jaune) --- (prune) --- Similarity: 0.318\n",
      "(Sandiwch Pollo Light) --- ( coockiw) --- Similarity: 0.297\n",
      "(Kalifornische Mandeln) --- (greek non fat yogurt) --- Similarity: 0.270\n",
      "(Organic mango chunks) --- (frozen mango chunks) --- Similarity: 0.841\n",
      "(Evolution fresh) --- (chestnut puree) --- Similarity: 0.350\n",
      "(Tzatziki) --- (tzatziki) --- Similarity: 1.000\n",
      "(Tomates grappes bio) --- (boneless pork filet) --- Similarity: 0.221\n",
      "(Chocolat Excellence Noir Intense 70%) --- (chocolate bar) --- Similarity: 0.390\n",
      "(Salami suisse) --- (salami) --- Similarity: 0.780\n",
      "(Organic Original Soy Non-Dairy Beverage) --- (soymilk) --- Similarity: 0.632\n",
      "(Gummi worms) --- (gummy worms) --- Similarity: 0.895\n",
      "(Lardons fumé) --- (red enchilada sauce) --- Similarity: 0.342\n",
      "(Meat Ball Stew) --- (meatballs) --- Similarity: 0.737\n",
      "(Sardines) --- (sardines) --- Similarity: 1.000\n",
      "(Whey protein isolate) --- (whey protein) --- Similarity: 0.933\n",
      "(Whey isolado) --- (whey protein) --- Similarity: 0.555\n",
      "(Veggie burgers) --- (veggie burgers) --- Similarity: 1.000\n",
      "(Mirtilli del Piemonte) --- (Italian olives) --- Similarity: 0.382\n",
      "(Filet mignon de porc) --- (espresso coffee ) --- Similarity: 0.249\n"
     ]
    }
   ],
   "source": [
    "from entity_linking import find_most_similar_pairs, read_specified_columns\n",
    "\n",
    "file1_path = \"../csv_file/entity_linking_test.csv\"\n",
    "\n",
    "lists = read_specified_columns(file_path=file1_path, elenco_colonne=[\"off\", \"foodkg\"], delimiter=\",\")\n",
    "\n",
    "list1 = [item[0] for item in lists]\n",
    "list2 = [item[1] for item in lists]\n",
    "\n",
    "most_similar_pairs = find_most_similar_pairs(list1, list2)\n",
    "\n",
    "print(\"Most similar couples:\\n\")\n",
    "for item1, item2, score in most_similar_pairs:\n",
    "    print(f\"({item1}) --- ({item2}) --- Similarity: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNIVERSITY OF BARI METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from entity_linking import (\n",
    "    RecipeTransformer,\n",
    "    compute_embeddings,\n",
    "    find_similar_by_title,\n",
    "    read_specified_columns,\n",
    ")\n",
    "\n",
    "file1_path = \"../csv_file/entity_linking_test.csv\"\n",
    "\n",
    "lists = read_specified_columns(file_path=file1_path, elenco_colonne=[\"off\", \"foodkg\"], delimiter=\",\")\n",
    "\n",
    "list1 = [item[0] for item in lists]\n",
    "list2 = [item[1] for item in lists]\n",
    "\n",
    "\n",
    "# Initialize the transformer\n",
    "transformer_name = \"davanstrien/autotrain-recipes-2451975973\"\n",
    "transformer = RecipeTransformer(transformer_name)\n",
    "\n",
    "# Compute embeddings for all recipes in list2\n",
    "print(\"Calculating embeddings for list2...\")\n",
    "embeddings2 = compute_embeddings(list2, transformer)\n",
    "\n",
    "# Create a list of tuples (index, title) for list2\n",
    "entities_list2 = list(enumerate(iterable=list2))\n",
    "\n",
    "# Find the most similar recipe for each item in list1\n",
    "most_similar_pairs = []\n",
    "print(\"Searching for the most similar recipes...\")\n",
    "for recipe_title in tqdm(list1, desc=\"Similarity search\"):\n",
    "    similar_recipe, similarity_score = find_similar_by_title(\n",
    "        recipe_title, entities_list2, embeddings2, transformer\n",
    "    )\n",
    "    most_similar_pairs.append((recipe_title, similar_recipe[1], similarity_score))\n",
    "\n",
    "# Output the results\n",
    "print(\"Most similar recipe pairs found:\\n\")\n",
    "for item1, item2, score in most_similar_pairs:\n",
    "    print(f\"({item1}) --------- ({item2}) --------- Similarity: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBRID METHOD WITH INDICATOR TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual contribution [(0.43465912342071533, 'Pasta', 'Pizza', 12, 10), (0.3310669958591461, 'Pane', 'Pizza', 12, 10)]\n",
      "negative contribution [(0.3560877740383148, 'Pasta', 'Pizza', 50, 0), (0.30325934290885925, 'Pane', 'Pizza', 50, 0)]\n",
      "positive contribution [(0.44269031286239624, 'Pasta', 'Pizza', 33, 33), (0.30325934290885925, 'Pane', 'Pizza', 33, 0)]\n",
      "no contribution [(0.3926903009414673, 'pasta', 'pizza', 0, 0), (0.30325934290885925, 'pane', 'pizza', 0, 0)]\n"
     ]
    }
   ],
   "source": [
    "from entity_linking import find_k_most_similar_pairs_with_indicators\n",
    "\n",
    "list1 = [(\"Pasta\", 30, 5, 10, \"Pasta\"), (\"Pane\", 50, 1, 10, \"Pane\")]\n",
    "list2 = [(\"Riso\", 40, 2, 8, \"Riso\"), (\"Pizza\", 20, 10, 12, \"Pizza\")]\n",
    "result = find_k_most_similar_pairs_with_indicators(list1, list2, use_indicator=True)\n",
    "print(\"actual contribution\", result)\n",
    "\n",
    "list1 = [(\"Pasta\", 100, 0, 0, \"Pasta\"), (\"Pane\", 0, 0, 0, \"Pane\")]\n",
    "list2 = [(\"Riso\", 0, 2, 8, \"Riso\"), (\"Pizza\", 0, 50, 50, \"Pizza\")]\n",
    "result = find_k_most_similar_pairs_with_indicators(list1, list2, use_indicator=True)\n",
    "print(\"negative contribution\", result)\n",
    "\n",
    "list1 = [(\"Pasta\", 33, 33, 33, \"Pasta\"), (\"Pane\", 0, 0, 0, \"Pane\")]\n",
    "list2 = [(\"Riso\", 0, 2, 8, \"Riso\"), (\"Pizza\", 33, 33, 33 , \"Pizza\")]\n",
    "result = find_k_most_similar_pairs_with_indicators(list1, list2, use_indicator=True)\n",
    "print(\"positive contribution\", result)\n",
    "\n",
    "list1 = [(\"pasta\", \"pasta\"), (\"pane\", \"pane\")]\n",
    "list2 = [(\"riso\", \"riso\"), (\"pizza\", \"pizza\")]\n",
    "result = find_k_most_similar_pairs_with_indicators(list1, list2)\n",
    "print(\"no contribution\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST OF VARIOUS BERT MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search of the best threshold value for the bert on a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entity_linking import read_specified_columns, evaluate_entity_linking_method\n",
    "import csv\n",
    "\n",
    "file_path = \"../csv_file/entity_linking_test_normalized_validation.csv\"\n",
    "column_list = [\"off_normalized\", \"foodkg_normalized\"]\n",
    "data = read_specified_columns(file_path, elenco_colonne=column_list, delimiter=\",\")\n",
    "\n",
    "#https://huggingface.co/spaces/mteb/leaderboard 09/12/2024\n",
    "list_of_models = [\n",
    "    \n",
    "    #top 5 in pair classification (around 10000000 parameter)\n",
    "    #voyage is not free\n",
    "    #\"meta-llama/Meta-Llama-3-8B-Instruct\", # have problem with the token's padding\n",
    "    \"nvidia/NV-Embed-v2\",\n",
    "    \"Salesforce/SFR-Embedding-Mistral\",\n",
    "    \"compressa-ai/Compressa-Embeddings\",\n",
    "\n",
    "    # top 3 under 1000000 parameters\n",
    "    \"dunzhang/stella_en_400M_v5\",\n",
    "    \"llmrails/ember-v1\",\n",
    "    \"WhereIsAI/UAE-Large-V1\",\n",
    "\n",
    "    # top 3 under 100000 parameters\n",
    "    \"infgrad/stella-base-en-v2\",\n",
    "    \"intfloat/e5-small\",\n",
    "    \"BAAI/bge-small-en-v1.5\", \n",
    "\n",
    "    #top 5 overall\n",
    "    #nvidia/NV-Embed-v2 alredy tested\n",
    "    \"dunzhang/stella_en_1.5B_v5\",\n",
    "    \"BAAI/bge-en-icl\",\n",
    "    \"blevlabs/stella_en_v5\",\n",
    "    \"Salesforce/SFR-Embedding-2_R\",\n",
    "\n",
    "    # top 3 in sts\n",
    "    \"Lajavaness/bilingual-embedding-large\",\n",
    "    \"ilhamdprastyo/jina-embeddings-v3-tei\",\n",
    "    \"jinaai/jina-embeddings-v3\"\n",
    "    ]\n",
    "\n",
    "\n",
    "column_names = [\"model_name\", \"vocab_size\", \"number_of_parameters\", \"accuracy\", \"accuracy_on_considered\", \"number_of_TP_and_TN\", \"threshold\", ]\n",
    "threshold = [(i/100) for i in  range(80, 100, 1)]\n",
    "\n",
    "output_file = \"../csv_file/bert_comparison_validation.csv\"\n",
    "\n",
    "with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(column_names)\n",
    "    \n",
    "    for model in list_of_models:\n",
    "        model_name, vocab_size, number_of_parameters, accuracy, accuracy_considered, number_of_TP_and_TN, threshold = evaluate_entity_linking_method(\n",
    "            data, show_progress=False, model=model, threshold_list=threshold\n",
    "        )\n",
    "        for modelz, vocab_sizez, number_of_parametersz, accuracyz, accuracy_consideredz, number_of_TP_and_TNz, thresholdz,  in zip(model_name, vocab_size, number_of_parameters, accuracy, accuracy_considered, number_of_TP_and_TN, threshold):\n",
    "            writer.writerow([model, vocab_sizez, number_of_parametersz, round(accuracyz, 2), round(accuracy_consideredz) , number_of_TP_and_TNz, thresholdz])\n",
    "\n",
    "print(f\"file created {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "determine the best bert on a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entity_linking import read_specified_columns, evaluate_entity_linking_method\n",
    "import csv\n",
    "\n",
    "file_path = \"../csv_file/entity_linking_test_normalized_test.csv\"\n",
    "column_list = [\"off_normalized\", \"foodkg_normalized\"]\n",
    "data = read_specified_columns(file_path, elenco_colonne=column_list, delimiter=\",\")\n",
    "\n",
    "file1_path = \"../csv_file/bert_comparison_validation.csv\"\n",
    "column_list = [\"model_name\", \"threshold\", \"accuracy_on_considered\"]\n",
    "reader = read_specified_columns(file1_path, elenco_colonne=column_list, delimiter=\",\")\n",
    "\n",
    "list_of_models = []\n",
    "list_of_threshold = []\n",
    "list_of_accuracy = []\n",
    "\n",
    "for model, threshold, accuracy in reader:\n",
    "    list_of_models.append(model)\n",
    "    list_of_threshold.append(threshold)\n",
    "    list_of_accuracy.append(accuracy)\n",
    "\n",
    "model_threshold_dictionary = {}\n",
    "\n",
    "for model, threshold, accuracy in zip(list_of_models, list_of_threshold, list_of_accuracy):\n",
    "    if model not in model_threshold_dictionary:\n",
    "        model_threshold_dictionary[model] = [threshold, accuracy]\n",
    "    else:\n",
    "        if ((float(accuracy) > float(model_threshold_dictionary[model][1])) | ((float(accuracy) == float(model_threshold_dictionary[model][1])) & (float(threshold) < float(model_threshold_dictionary[model][0])))):\n",
    "            model_threshold_dictionary[model] = [threshold, accuracy]\n",
    "\n",
    "print(model_threshold_dictionary)\n",
    "\n",
    "\n",
    "\n",
    "column_names = [\"model_name\", \"vocab_size\", \"number_of_parameters\", \"accuracy\", \"accuracy_on_considered\", \"number_of_TP_and_TN\", \"threshold\", ]\n",
    "\n",
    "output_file = \"../csv_file/bert_comparison_on_test_set.csv\"\n",
    "\n",
    "with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(column_names)\n",
    "    for model in model_threshold_dictionary.keys():\n",
    "        threshold = model_threshold_dictionary[model][0]\n",
    "        try:\n",
    "            threshold_list = [float(threshold)]\n",
    "        except ValueError:\n",
    "            print(f\"Invalid threshold for model {model}: {threshold}\")\n",
    "            continue  \n",
    "\n",
    "        model_name, vocab_size, number_of_parameters, accuracy, accuracy_considered, number_of_TP_and_TN, threshold = evaluate_entity_linking_method(\n",
    "            data, show_progress=False, model=model, threshold_list=threshold_list\n",
    "        )\n",
    "        for modelz, vocab_sizez, number_of_parametersz, accuracyz, accuracy_consideredz, number_of_TP_and_TNz, thresholdz in zip(\n",
    "            model_name, vocab_size, number_of_parameters, accuracy, accuracy_considered, number_of_TP_and_TN, threshold\n",
    "        ):\n",
    "            writer.writerow([modelz, vocab_sizez, number_of_parametersz, round(accuracyz, 2), round(accuracy_consideredz), number_of_TP_and_TNz, thresholdz])\n",
    "\n",
    "print(f\"file created {output_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting the file creation\n",
      "\n",
      "\n",
      "number hummus recipe  9358\n",
      "number off recipe:  7122\n",
      "number recipe foodkg:  5434\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from entity_linking import read_specified_columns\n",
    "\n",
    "header = [\"name_normalized\"]\n",
    "file_off = \"../csv_file/off_recipe_for_linking.csv\"\n",
    "file_hummus = \"../csv_file/hum_recipe_for_linking.csv\"\n",
    "file_foodkg = \"../csv_file/foodkg_recipe_for_linking.csv\"\n",
    "\n",
    "print(f\"starting the file creation\\n\\n\")\n",
    "\n",
    "# Columns of the hummus file to be used for the merging\n",
    "hummus_file_path = \"../csv_file/pp_recipes_normalized_by_pipeline.csv\"\n",
    "hummus_column: list[str] = [\n",
    "    \"title_normalized\"\n",
    "]\n",
    "list_hummus_recipe = read_specified_columns(\n",
    "    hummus_file_path, hummus_column, delimiter=\";\"\n",
    ")\n",
    "\n",
    "seen = set()\n",
    "unique_recipe = []\n",
    "for row in list_hummus_recipe:\n",
    "    key = row  \n",
    "    if key != \"\" and key is not None and key not in seen:  \n",
    "        seen.add(key)\n",
    "        unique_recipe.append(row)\n",
    "\n",
    "list_hummus_recipe = unique_recipe\n",
    "\n",
    "print(\"number hummus recipe \", len(list_hummus_recipe))\n",
    "\n",
    "with open(file_hummus, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "    for name1 in list_hummus_recipe:\n",
    "        writer.writerow(name1)\n",
    "\n",
    "\n",
    "# Columns of the off file to be used for the merging\n",
    "off_file_path = \"../csv_file/off_normalized_final.csv\"\n",
    "off_column = [\n",
    "    \"product_name_normalized\",\n",
    "]\n",
    "list_off_recipe = read_specified_columns(\n",
    "    off_file_path, off_column, delimiter=\"\\t\"\n",
    ")\n",
    "\n",
    "seen = set()\n",
    "unique_recipe = []\n",
    "for row in list_off_recipe:\n",
    "    key = row \n",
    "    if key != \"\" and key is not None and key not in seen:  \n",
    "        seen.add(key)\n",
    "        unique_recipe.append(row)\n",
    "\n",
    "list_off_recipe = unique_recipe\n",
    "\n",
    "print(\"number off recipe: \", len(list_off_recipe))\n",
    "\n",
    "with open(file_off, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "    for name1 in list_off_recipe:\n",
    "        writer.writerow(name1)\n",
    "\n",
    "\n",
    "\n",
    "# Columns of the off file to be used for the merging\n",
    "foodkg_file_path = \"../csv_file/ingredients_food_kg_normalizzed_by_pipeline.csv\"\n",
    "foodkg_column = [\n",
    "    \"ingredient_normalized\",\n",
    "]\n",
    "list_foodkg_recipe = read_specified_columns(\n",
    "    foodkg_file_path, foodkg_column, delimiter=\",\"\n",
    ")\n",
    "\n",
    "seen = set()\n",
    "unique_recipe = []\n",
    "for row in list_foodkg_recipe:\n",
    "    key = row  \n",
    "    if key != \"\" and key is not None and key not in seen: \n",
    "        seen.add(key)\n",
    "        unique_recipe.append(row)\n",
    "\n",
    "list_foodkg_recipe = unique_recipe\n",
    "\n",
    "print(\"number recipe foodkg: \", len(list_foodkg_recipe))\n",
    "\n",
    "with open(file_foodkg, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "    for name1 in list_foodkg_recipe:\n",
    "        writer.writerow(name1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foodnexus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
