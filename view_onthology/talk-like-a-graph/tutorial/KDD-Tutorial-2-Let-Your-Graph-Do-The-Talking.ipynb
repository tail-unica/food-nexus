{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is a noteboook that illustrates how to use Graph Neural Networks to encode structured data for use in Large Language Models. It is Part 2 of a two part tutorial from KDD'24.\n",
        "\n",
        "**This notebook requires a TPUv2 runtime**\n",
        "\n",
        "If you find this tutorial useful or want to know more, please consider our publication:\n",
        "Let your graph do the talking: Encoding structured data for LLMs\n",
        "```\n",
        "@article{perozzi2024let,\n",
        "  title={Let your graph do the talking: Encoding structured data for llms},\n",
        "  author={Perozzi, Bryan and Fatemi, Bahare and Zelle, Dustin and Tsitsulin, Anton and Kazemi, Mehran and Al-Rfou, Rami and Halcrow, Jonathan},\n",
        "  journal={arXiv preprint arXiv:2402.05862},\n",
        "  year={2024}\n",
        "}\n",
        "```\n",
        "\n",
        "## Tutorial Part II: GNN Encoding of Graph Information\n",
        "This notebook takes the work we did in the first part of the tutorial and extends it to using a Graph Neural Network to directly encode a representation of a graph into a prompt (vs using a text encoding as we did in the previous part).\n",
        "\n",
        "## Notebook Outline:\n",
        "\n",
        "Setup (Install Dependencies, download Gemma weights)\n",
        "Dataset creation\n",
        "Graph-to-Text conversion\n",
        "Evaluation\n",
        "Exercise: Graph Encoding Challenge\n",
        "Exercise: DBLP Dataset\n",
        "Setup\n",
        "\n",
        "## Prework!\n",
        "\n",
        "Sign-up for Kaggle and consent to the Gemma TOS (this is a requirement to download the Gemma weights used in this notebook).\n",
        "https://www.kaggle.com/models/google/gemma/license/consent?returnUrl=%2Fmodels%2Fgoogle%2Fgemma%2FFlax%2F2b-it%2F2"
      ],
      "metadata": {
        "id": "EdaaBjBQ3c5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# @title Install Dependencies\n",
        "!pip install git+https://github.com/google-deepmind/gemma.git\n",
        "!pip install --user kaggle\n",
        "!pip install sparse_deferred\n",
        "!git clone https://github.com/google-research/talk-like-a-graph.git\n",
        "import sys\n",
        "sys.path.insert(0, \"/content/talk-like-a-graph\")\n"
      ],
      "metadata": {
        "id": "BtYsxQuUQ-K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Login to Kaggle\n",
        "Follow the link in the login dialog to get an API key if you don't already have one. Also make sure to approve the [Gemma TOS](https://www.kaggle.com/models/google/gemma/license/consent?returnUrl=%2Fmodels%2Fgoogle%2Fgemma%2FFlax%2F2b-it%2F2) as well."
      ],
      "metadata": {
        "id": "42csum1-l0bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "kagglehub.login()"
      ],
      "metadata": {
        "id": "xmM6CSh7RbIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Gemma"
      ],
      "metadata": {
        "id": "500UNvQOmcMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "VARIANT = '2b-it' # @param ['2b', '2b-it', '7b', '7b-it'] {type:\"string\"}\n",
        "weights_dir = kagglehub.model_download(f'google/gemma/Flax/{VARIANT}')\n",
        "ckpt_path = os.path.join(weights_dir, VARIANT)\n",
        "vocab_path = os.path.join(weights_dir, 'tokenizer.model')"
      ],
      "metadata": {
        "id": "W0BzTj2tPROY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import dependencies"
      ],
      "metadata": {
        "id": "WGFZHBaCRa77"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycYOnVRpHZuJ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import os\n",
        "from collections.abc import Sequence\n",
        "import dataclasses\n",
        "from typing import Any, Callable, Mapping\n",
        "import sys\n",
        "\n",
        "\n",
        "import chex\n",
        "from flax import linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from gemma import params as params_lib\n",
        "from gemma import transformer as transformer_lib\n",
        "from gemma import sampler as sampler_lib\n",
        "import sentencepiece as spm\n",
        "import sparse_deferred as sd\n",
        "from sparse_deferred import jax as sdjnp\n",
        "from sparse_deferred.structs import graph_struct\n",
        "from sparse_deferred import np as sdnp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GraphToken library code"
      ],
      "metadata": {
        "id": "DzQUN-bCmrjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import collections\n",
        "from collections.abc import Iterable\n",
        "import io\n",
        "import json\n",
        "from typing import Any, Callable, NamedTuple, Sequence\n",
        "\n",
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "# Code for converting NetworkX graphs to graph tensor\n",
        "def laplacian_pos_embedding(graph: nx.Graph, units: int = 4) -\u003e nx.Graph:\n",
        "  \"\"\"Adds the laplacian positional encoding.\"\"\"\n",
        "  m = nx.normalized_laplacian_matrix(\n",
        "      graph, nodelist=sorted(graph.nodes), weight=None\n",
        "  ).astype(np.float32)\n",
        "  u, _, _ = np.linalg.svd(m.todense(), compute_uv=True)\n",
        "  if units \u003e u.shape[1]:\n",
        "    u = np.pad(u, ((0, 0), (0, units - u.shape[1])))\n",
        "  nx.set_node_attributes(\n",
        "      graph, dict(zip(sorted(graph.nodes), u[:, :units])), name='lpe'\n",
        "  )\n",
        "  return graph\n",
        "\n",
        "\n",
        "def to_graph_struct(graph: nx.Graph, node_ids: list[int]=None) -\u003e graph_struct.GraphStruct:\n",
        "  if graph.edges(data=True):\n",
        "    s, t, w = zip(*[\n",
        "        (s, t, (d['weight'] if d and 'weight' in d else None))\n",
        "        for s, t, d in graph.edges(data=True)\n",
        "    ])\n",
        "  else:\n",
        "    s, t, w = (), (), ()\n",
        "  # tfgnn assumes graphs are directed. Adding the rev edges for an undirected\n",
        "  # graph.\n",
        "  if not graph.is_directed():\n",
        "    s, t, w = s + t, t + s, w + w\n",
        "\n",
        "  graph = laplacian_pos_embedding(graph, units=4)\n",
        "  return graph_struct.GraphStruct.new(\n",
        "    nodes={'nodes': {'lpe': np.stack([graph.nodes('lpe')[i] for i in range(graph.number_of_nodes())])}},\n",
        "    edges={'edges': ((np.array(s, dtype=np.int32), np.array(t, dtype=np.int32)), {})}\n",
        "  )\n",
        "\n",
        "\n",
        "Tensor = sd.matrix.Tensor\n",
        "Features = dict[str, Tensor]\n",
        "FeatureSets = dict[str, Features]\n",
        "Edge = tuple[tuple[Tensor, ...], Features]  # (endpoints, edge features)\n",
        "Edges = dict[str, Edge]\n",
        "Nodes = FeatureSets\n",
        "Schema = dict[str, tuple[str, ...]]\n",
        "_Schema = dict[str, tuple[dict[str, int], ...]]\n",
        "\n",
        "\n",
        "\n",
        "class FixedSizePadder:\n",
        "  \"\"\"Adds padding to `GraphStruct` instances for fixed-sized tensors.\n",
        "\n",
        "  Fixed-size tensors can be preferred when running on TPU accelerators.\n",
        "\n",
        "  To use this class, you must first initialize it with statistics of your graphs\n",
        "  then use it to pad graphs. The statistics can be initialized by invoking\n",
        "  `calculate_pad_statistics`: this function records the *maximum* observerd size\n",
        "  of every node and edge set, as well as the standard deviation (std) of sizes.\n",
        "\n",
        "  Once initialized, the function: `pad_graph()` will add padding to the graph.\n",
        "  Specifically, the node feature (tensors) will be padded with zeros. Similarly,\n",
        "  edges will be inserted, among newly-added virtual nodes.\n",
        "\n",
        "  Each node (or edge) size will become:\n",
        "\n",
        "  `max observed [per calculate_pad_statistics] + slack*std + 1`\n",
        "\n",
        "  NOTE: there will always be at least one more node or edge, even if the\n",
        "  statistics show zero std. This is required for making virtual nodes.\n",
        "\n",
        "  All sizes node-set (features) and edge-set (features and adjacency list)\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, engine: sd.ComputeEngine, slack: float = 1.0):\n",
        "    # `('edge'|'node', NodeOrEdgeName) -\u003e target size`\n",
        "    # where `target size` is maximum observed size for node (or edge) set, plus\n",
        "    # one, plus slack-times-std of observed sizes.\n",
        "    self.sizes: dict[tuple[str, str], int] = {}\n",
        "    self.slack = slack\n",
        "    self._engine = engine\n",
        "\n",
        "  def calculate_pad_statistics(\n",
        "      self, examples: Iterable[graph_struct.GraphStruct], num_steps: int = 100):\n",
        "    \"\"\"Measures the max and std of node \u0026 edge sizes of elements of `examples`.\n",
        "\n",
        "    Calling this function is necessary before invoking `pad_graph`.\n",
        "\n",
        "    Args:\n",
        "      examples: iterable that yields `GraphStruct` examples.\n",
        "      num_steps: If positive, considers this many samples of `examples`.\n",
        "        Otherwise, iterates over all `examples`. Warning: this may run\n",
        "        infinitely on infinite iterators (e.g., `dataset.repeat()`).\n",
        "    \"\"\"\n",
        "    sizes: dict[tuple[str, str], list[int]] = collections.defaultdict(list)\n",
        "    for i, graph in enumerate(examples):\n",
        "      assert isinstance(graph, graph_struct.GraphStruct)\n",
        "      if i \u003e 0 and i \u003e= num_steps:\n",
        "        break\n",
        "      for node_name, features in graph.nodes.items():\n",
        "        value_list = sizes[('nodes', node_name)]\n",
        "        if not features:\n",
        "          value_list.append(0)\n",
        "        else:\n",
        "          value_list.append(list(features.values())[0].shape[0])\n",
        "\n",
        "      for edge_name, edges_tuple in graph.edges.items():\n",
        "        value_list = sizes[('edges', edge_name)]\n",
        "        source_nodes = edges_tuple[0][0]\n",
        "        # if len(value_list) and edge_set.sizes.shape != value_list[-1].shape:\n",
        "        #   continue\n",
        "        value_list.append(source_nodes.shape[0])\n",
        "\n",
        "    self.sizes = {k: int(1 + max(v) + self.slack * np.std(v))\n",
        "                  for k, v in sizes.items()}\n",
        "\n",
        "  def pad_graph(self, graph: graph_struct.GraphStruct) -\u003e graph_struct.GraphStruct:\n",
        "    \"\"\"Pads node-sets and edge-sets, with zeros, to max-seen during `calc..`.\n",
        "\n",
        "    This function is useful for running on TPU hardware.\n",
        "\n",
        "    Args:\n",
        "      graph: contains any number of nodes and edges.\n",
        "\n",
        "    Returns:\n",
        "      graph with deterministic number of nodes and edges. See class docstring.\n",
        "    \"\"\"\n",
        "    if not self.sizes:\n",
        "      raise ValueError(\n",
        "          'No statistics have been initialized. '\n",
        "          'Perhaps you forgot to invoke \"calculate_pad_statistics\"?')\n",
        "    # Edge set name -\u003e (1D vectors containing endpoints**), {\"feature\": Tensor})\n",
        "    edges: Edges = {}\n",
        "    # ** tuple should have 2 entries for directed graphs\n",
        "\n",
        "    nodes: Nodes = {}\n",
        "\n",
        "    # For every key in `edges`, store names of node sets that `key` edge\n",
        "    # connects.\n",
        "    schema = graph.schema\n",
        "\n",
        "    e = self._engine  # for short.\n",
        "    for node_name, node_features in graph.nodes.items():\n",
        "      padded_features = {}\n",
        "      desired_size = self.sizes[('nodes', node_name)]\n",
        "\n",
        "      for feature_name, feature in node_features.items():\n",
        "        feature = feature[:desired_size]  # if `is_oversized`.\n",
        "        pad = self._engine.maximum(\n",
        "            desired_size - self._engine.shape(feature)[0], 0)\n",
        "        zeros = e.zeros(\n",
        "            tuple([pad] + list(feature.shape[1:])), dtype=feature.dtype)\n",
        "        padded_feature = e.concat([feature, zeros], axis=0)\n",
        "        padded_feature = e.reshape(\n",
        "            padded_feature, [desired_size] + list(padded_feature.shape[1:]))\n",
        "        padded_features[feature_name] = padded_feature\n",
        "\n",
        "      nodes[node_name] = padded_features\n",
        "\n",
        "    for edge_name, (edge_endpoints, features) in graph.edges.items():\n",
        "      padded_features = {}\n",
        "      padded_endpoints = []\n",
        "      desired_size = self.sizes[('edges', edge_name)]\n",
        "      current_size = e.shape(edge_endpoints[0])[0]\n",
        "\n",
        "      pad = e.maximum(desired_size - current_size, 0)\n",
        "      e.assert_greater(pad, -1)\n",
        "\n",
        "      for feature_name, feature in features.items():\n",
        "        feature = feature[:desired_size]  # if `is_oversized`.\n",
        "        zeros = e.zeros(\n",
        "            tuple([pad] + list(feature.shape[1:])), dtype=feature.dtype\n",
        "        )\n",
        "        padded_feature = e.concat([feature, zeros], axis=0)\n",
        "        padded_feature = e.reshape(\n",
        "            padded_feature, [desired_size] + list(padded_feature.shape[1:])\n",
        "        )\n",
        "        padded_features[feature_name] = padded_feature\n",
        "\n",
        "      edge_endpoints = [node_ids[:desired_size] for node_ids in edge_endpoints]\n",
        "      # [[src1_is_valid, src2_is_valid, ...], [tgt1_is_valid, ...]]\n",
        "      valid = e.cast(\n",
        "          [\n",
        "              ids \u003c self.sizes[('nodes', node_name)]\n",
        "              for ids, node_name in zip(edge_endpoints, schema[edge_name])\n",
        "          ],\n",
        "          dtype=bool,\n",
        "      )\n",
        "      valid = e.reduce_all(valid, axis=0)\n",
        "\n",
        "      for node_ids, node_name in zip(edge_endpoints, schema[edge_name]):\n",
        "        # Universe size (e.g., of source or target).\n",
        "        max_endpoint = self.sizes[('nodes', node_name)] - 1\n",
        "        node_ids = node_ids[:desired_size]\n",
        "        node_ids = e.boolean_mask(node_ids, valid)\n",
        "        pad = desired_size - e.shape(node_ids)[0]  # Need only to compute once.\n",
        "\n",
        "        padded_ids = e.concat([\n",
        "            node_ids,\n",
        "            e.ones((pad), dtype=node_ids.dtype) * max_endpoint\n",
        "        ], axis=0)\n",
        "        padded_ids = e.reshape(padded_ids, [desired_size])\n",
        "        padded_endpoints.append(padded_ids)\n",
        "\n",
        "      edges[edge_name] = (tuple(padded_endpoints), padded_features)\n",
        "\n",
        "    graph = graph_struct.GraphStruct.new(nodes=nodes, edges=edges, schema=schema)\n",
        "    return graph\n",
        "\n",
        "\n",
        "\n",
        "## gnn.py\n",
        "class GIN(nn.Module):\n",
        "  \"\"\"Graph Isomorphism Network: https://arxiv.org/pdf/1810.00826.pdf.\"\"\"\n",
        "\n",
        "  output_dim: int\n",
        "  num_hidden_layers: int = 1\n",
        "  hidden_dim: int = 32\n",
        "  epsilon: float = 0.1  # See GIN paper (link above)\n",
        "\n",
        "  def setup(self):\n",
        "    layer_dims = [self.hidden_dim] * self.num_hidden_layers\n",
        "    self.layers = [\n",
        "        nn.Dense(dim, use_bias=False, dtype=jnp.bfloat16) for dim in layer_dims\n",
        "    ]\n",
        "    self.out_layer = nn.Dense(\n",
        "        self.output_dim, use_bias=False, dtype=jnp.bfloat16\n",
        "    )\n",
        "\n",
        "  def __call__(self, graph: graph_struct.GraphStruct) -\u003e jax.Array:\n",
        "    x = graph.nodes['nodes']['lpe']\n",
        "    adj = graph.adj(sdjnp.engine, 'edges')\n",
        "    adj = adj.add_eye(1 + self.epsilon)  # self connections with 1+eps weight.\n",
        "\n",
        "    for i, layer in enumerate(self.layers):\n",
        "      x = layer(adj @ x)\n",
        "      if i \u003c self.num_hidden_layers:\n",
        "        x = nn.relu(x)\n",
        "    x = jnp.concat(x, axis=-1)\n",
        "    return self.out_layer(x)\n",
        "\n",
        "\n",
        "class GCN(nn.Module):\n",
        "  \"\"\"Graph convolutional network: https://arxiv.org/pdf/1609.02907.pdf.\"\"\"\n",
        "\n",
        "  output_dim: int\n",
        "  num_hidden_layers: int = 1\n",
        "  hidden_dim: int = 32\n",
        "\n",
        "  def setup(self):\n",
        "    layer_dims = [self.hidden_dim] * self.num_hidden_layers\n",
        "    self.layers = [nn.Dense(dim, use_bias=False) for dim in layer_dims]\n",
        "    self.out_layer = nn.Dense(\n",
        "        self.output_dim, use_bias=False, dtype=jnp.bfloat16\n",
        "    )\n",
        "\n",
        "  def __call__(self, graph: graph_struct.GraphStruct) -\u003e jax.Array:\n",
        "    x = graph.nodes['nodes']['lpe']\n",
        "    adj = graph.adj(sdjnp.engine, 'edges')\n",
        "    adj_symnorm = (adj + adj.transpose()).add_eye().normalize_symmetric()\n",
        "\n",
        "    for i, layer in enumerate(self.layers):\n",
        "      x = layer(adj_symnorm @ x)\n",
        "      if i \u003c self.num_hidden_layers:\n",
        "        x = nn.relu(x)\n",
        "    x = jnp.concat(x, axis=-1)\n",
        "    return self.out_layer(x)\n",
        "\n",
        "\n",
        "## sampler.py\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class SamplerOutput:\n",
        "\n",
        "  # Decoded samples from the model.\n",
        "  text: list[str]\n",
        "\n",
        "  # Per-step logits used during sampling.\n",
        "  logits: list[list[float]]\n",
        "\n",
        "  # Tokens corresponding to the generated samples.\n",
        "  tokens: list[list[int]]\n",
        "\n",
        "  graph_embeddings: list[jnp.ndarray]\n",
        "\n",
        "\n",
        "class GraphTokenSampler:\n",
        "  \"\"\"Sampler for GraphToken.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      gnn: nn.Module,\n",
        "      llm: transformer_lib.Transformer,\n",
        "      vocab: spm.SentencePieceProcessor,\n",
        "      params: Mapping[str, Any],\n",
        "      gnn_token_template: str = r'\u003cunused%d\u003e',\n",
        "  ):\n",
        "    \"\"\"Initializes the sampler.\n",
        "\n",
        "    Args:\n",
        "      gnn: The GNN model.\n",
        "      llm: The LLM model.\n",
        "      vocab: The vocab used by the LLM.\n",
        "      params: The parameters for the GNN and LLM. This should contain the params\n",
        "        for the gnn under params['gnn'] and the params for the llm under\n",
        "        params['transformer']\n",
        "      gnn_token_template: The token used to represent the GNN embedding.\n",
        "    \"\"\"\n",
        "\n",
        "    self._gnn = gnn\n",
        "    self._llm = llm\n",
        "    self._params = params\n",
        "    self._vocab = vocab\n",
        "    self._gnn_token_template = gnn_token_template\n",
        "    self._sampler = sampler_lib.Sampler(\n",
        "        transformer=self._llm,\n",
        "        vocab=self._vocab,\n",
        "        params=self._params['transformer'],\n",
        "    )\n",
        "\n",
        "  def __call__(\n",
        "      self,\n",
        "      input_strings: Sequence[str],\n",
        "      input_graphs: Sequence[graph_struct.GraphStruct],\n",
        "      total_generation_steps: int,\n",
        "      echo: bool = False,\n",
        "      return_logits: bool = True,\n",
        "      forbidden_tokens: Sequence[str] | None = None,\n",
        "  ) -\u003e SamplerOutput:\n",
        "    \"\"\"Samples from the model.\n",
        "\n",
        "    Args:\n",
        "      input_strings: The input strings.\n",
        "      input_graphs: The input graphs.\n",
        "      total_generation_steps: The number of steps to generate.\n",
        "      echo: Whether to echo the input.\n",
        "      return_logits: Whether to return the logits.\n",
        "      forbidden_tokens: Tokens that are forbidden, in addition to the GNN token.\n",
        "\n",
        "    Returns:\n",
        "      The sampled output.\n",
        "    \"\"\"\n",
        "    assert len(input_graphs) == len(input_strings), (\n",
        "        len(input_graphs),\n",
        "        len(input_strings),\n",
        "    )\n",
        "    augmented_inputs = []\n",
        "    full_forbidden_tokens = []\n",
        "    if forbidden_tokens is not None:\n",
        "      full_forbidden_tokens += forbidden_tokens\n",
        "    graph_embeddings = []\n",
        "    augmented_transformer_params = self._params['transformer']\n",
        "\n",
        "    placeholder_token = PLACEHOLDER_TOKEN\n",
        "    full_forbidden_tokens.append(placeholder_token)\n",
        "    placeholder_token_id = self._vocab.EncodeAsIds(placeholder_token)\n",
        "    assert len(placeholder_token_id) == 1, placeholder_token\n",
        "    placeholder_token_id = placeholder_token_id[0]\n",
        "\n",
        "    for prompt, graph in zip(input_strings, input_graphs):\n",
        "      embed = self._gnn.apply(self._params['gnn'], graph)\n",
        "      assert (\n",
        "          self._params['transformer']['embedder']['input_embedding'][\n",
        "              placeholder_token_id\n",
        "          ].shape\n",
        "          == embed.shape\n",
        "      )\n",
        "      augmented_transformer_params['embedder']['input_embedding'] = (\n",
        "          augmented_transformer_params['embedder']['input_embedding']\n",
        "          .at[placeholder_token_id]\n",
        "          .set(embed)\n",
        "      )\n",
        "      graph_embeddings.append(embed)\n",
        "      augmented_inputs.append(placeholder_token + prompt)\n",
        "\n",
        "    self._sampler.params = augmented_transformer_params\n",
        "    o = self._sampler(\n",
        "        input_strings=augmented_inputs,\n",
        "        total_generation_steps=total_generation_steps,\n",
        "        echo=echo,\n",
        "        return_logits=return_logits,\n",
        "        forbidden_tokens=full_forbidden_tokens,\n",
        "    )\n",
        "    return SamplerOutput(\n",
        "        **dataclasses.asdict(o),\n",
        "        graph_embeddings=graph_embeddings,\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "@chex.dataclass(frozen=True)\n",
        "class TrainingInput:\n",
        "  \"\"\"Batch of training data for a GraphToken model.\"\"\"\n",
        "\n",
        "  # Input tokens given to the model\n",
        "  input_tokens: np.ndarray  # size [B, L]\n",
        "\n",
        "  # A mask that determines which tokens contribute to the target loss\n",
        "  # calculation.\n",
        "  target_mask: np.ndarray  # size [B, L]\n",
        "\n",
        "  input_graphs: list[graph_struct.GraphStruct]  # size [B]\n",
        "\n",
        "  # Ground truth for the input tokens, if representable as an integer.\n",
        "  # For boolean classification tasks, this is 0/1.\n",
        "  parsed_ground_truth: np.ndarray | None  # size [B]\n",
        "\n",
        "\n",
        "def parse_int(s: str) -\u003e int:\n",
        "  \"\"\"Parse a string as an integer.\"\"\"\n",
        "  return int(float(s.strip()))\n",
        "\n",
        "\n",
        "def parse_yes_no(s: str) -\u003e bool:\n",
        "  \"\"\"Parse a string as a yes/no answer, looking at the first 10 chars.\"\"\"\n",
        "  return 'yes' in s.lower()[:10]\n",
        "\n",
        "PLACEHOLDER_TOKEN = '\u003cunused0\u003e'\n",
        "\n",
        "def graphqa_ds(\n",
        "    vocab: spm.SentencePieceProcessor,\n",
        "    encoded_examples: list,\n",
        "    padder: graph_struct.FixedSizePadder | None = None,\n",
        "    max_tokens: int = 100,\n",
        "    gt_parser: Callable[[str], Any] | None = None,\n",
        ") -\u003e tuple[graph_struct.FixedSizePadder, list[TrainingInput]]:\n",
        "  \"\"\"Load a GraphQA dataset as a list of TrainingInput.\n",
        "\n",
        "  Args:\n",
        "    vocab: The vocab to use for tokenization.\n",
        "    encoded_examples: List of encoded examples generated by GraphQA\n",
        "    padder: The padder to use for padding the graph. If None, a new padder will\n",
        "      be created and returned. This is so a padder can be shared across multiple\n",
        "      datasets / splits.\n",
        "    max_tokens: The maximum number of tokens to allow in the input. For\n",
        "      'task_only' prompting this can be quite small (100 tokens is plenty)\n",
        "    gt_parser: A function to parse the ground truth from the answer string, used\n",
        "      to supply the 'parsed_ground_truth' field in the TrainingInput.\n",
        "\n",
        "  Returns:\n",
        "    The padder used for padding the graphs, and a list of TrainingInput.\n",
        "  \"\"\"\n",
        "\n",
        "  output = []\n",
        "  for ex in encoded_examples:\n",
        "      query = PLACEHOLDER_TOKEN + ex['question'][ex['question'].find('Q:'):]\n",
        "      answer = ex['answer']\n",
        "      graph = to_graph_struct(ex['graph'])\n",
        "      query_tokens = vocab.EncodeAsIds(query)\n",
        "      answer_tokens = vocab.EncodeAsIds(answer) + [vocab.eos_id()]\n",
        "      input_tokens = np.array([vocab.bos_id()] + query_tokens + answer_tokens)\n",
        "      target_mask = np.zeros_like(input_tokens, dtype=jnp.int32)\n",
        "      # Add one for BOS token\n",
        "      target_mask[len(query_tokens) + 1 :] = 1\n",
        "      orig_len = len(query_tokens) + len(answer_tokens) + 1\n",
        "      input_tokens = np.pad(\n",
        "          input_tokens,\n",
        "          [[0, max_tokens - orig_len]],\n",
        "          constant_values=vocab.pad_id(),\n",
        "      )\n",
        "\n",
        "      target_mask = np.pad(target_mask, [[0, max_tokens - orig_len]])\n",
        "\n",
        "\n",
        "      # The GNN library that we are using requires a global feature. We set\n",
        "      # a fake value here, but it is unused otherwise.\n",
        "      #graph = graph.update(nodes={'g': {'foo': np.zeros([1])}})\n",
        "\n",
        "      output.append(\n",
        "          TrainingInput(\n",
        "              input_tokens=np.array([input_tokens]),\n",
        "              target_mask=np.array([target_mask]),\n",
        "              input_graphs=[graph],\n",
        "              parsed_ground_truth=np.array(gt_parser(answer)) if gt_parser else None,\n",
        "          )\n",
        "      )\n",
        "  if padder is None:\n",
        "    padder = FixedSizePadder(sdnp.engine)\n",
        "    padder.calculate_pad_statistics(\n",
        "        [e.input_graphs[0] for e in output], len(output)\n",
        "    )\n",
        "  for o in output:\n",
        "    o.input_graphs[0] = padder.pad_graph(o.input_graphs[0])\n",
        "  return padder, output\n",
        "\n",
        "\n",
        "def decode_questions(\n",
        "    training_input: TrainingInput, vocab: spm.SentencePieceProcessor\n",
        ") -\u003e list[str]:\n",
        "  \"\"\"Decode the question from the input tokens. (ignoring the first 2).\"\"\"\n",
        "  b, l = training_input.input_tokens.shape\n",
        "  question_tokens = []\n",
        "  for i in range(b):\n",
        "    question_tokens.append([])\n",
        "    # Skip the first two tokens (BOS and control token).\n",
        "    for j in range(2, l):\n",
        "      if training_input.target_mask[i, j] == 1:\n",
        "        break\n",
        "      question_tokens[i].append(int(training_input.input_tokens[i, j]))\n",
        "  return [''.join(vocab.DecodeIds(q)) for q in question_tokens]\n",
        "\n",
        "\n",
        "## training_loop\n",
        "import functools\n",
        "from typing import Any, MutableMapping\n",
        "\n",
        "import chex\n",
        "from flax import linen as nn\n",
        "from gemma import transformer as transformer_lib\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import tqdm\n",
        "\n",
        "Params = MutableMapping[str, Any]\n",
        "\n",
        "\n",
        "def get_attention_mask_and_positions(\n",
        "    example: jax.Array,\n",
        "    pad_id: int,\n",
        ") -\u003e tuple[jax.Array, jax.Array]:\n",
        "  \"\"\"Builds the position and attention mask vectors from the given tokens.\"\"\"\n",
        "  pad_mask = example != pad_id\n",
        "  current_token_position = transformer_lib.build_positions_from_mask(pad_mask)\n",
        "  attention_mask = transformer_lib.make_causal_attn_mask(pad_mask)\n",
        "  return current_token_position, attention_mask\n",
        "\n",
        "\n",
        "def forward_and_loss_fn(\n",
        "    params: Params,\n",
        "    *,\n",
        "    gnn: nn.Module,\n",
        "    llm: transformer_lib.Transformer,\n",
        "    input_tokens: jax.Array,  # Shape [B, L]\n",
        "    input_graphs: list[graph_struct.GraphStruct],  # Shape [B]\n",
        "    input_mask: jax.Array,  # Shape [B, L]\n",
        "    positions: jax.Array,  # Shape [B, L]\n",
        "    attention_mask: jax.Array,  # [B, L, L]\n",
        "    placeholder_token_id: int,\n",
        ") -\u003e jax.Array:\n",
        "  \"\"\"Forward pass and loss function.\n",
        "\n",
        "  Args:\n",
        "    params: Params for the gnn and transformer. The gnn params are stored in\n",
        "      params['gnn'] and the llm params are stored in params['transformer'].\n",
        "    gnn: gnn model to call.\n",
        "    llm: gemma transformer model to call.\n",
        "    input_tokens: input tokens sequence, shape [B, L].\n",
        "    input_graphs: input graphs.\n",
        "    input_mask: tokens to ignore when computing the loss, shape [B, L].\n",
        "    positions: relative position of each token, shape [B, L].\n",
        "    attention_mask: input attention mask, shape [B, L].\n",
        "    placeholder_token_id: Index in the LLM vocabulary that we are using for passing\n",
        "      graph embeddings.\n",
        "\n",
        "  Returns:\n",
        "    Softmax cross-entropy loss for the next-token prediction task.\n",
        "  \"\"\"\n",
        "  # Right now we only support batch_size = 1\n",
        "  chex.assert_axis_dimension(input_tokens, 0, 1)\n",
        "  chex.assert_equal_shape([input_tokens, input_mask, positions])\n",
        "  chex.assert_axis_dimension(attention_mask, 0, 1)\n",
        "  chex.assert_equal(len(input_graphs), 1)\n",
        "\n",
        "  # Get the GNN embedding and update the transformer input embedding for a\n",
        "  # control token.\n",
        "  graph_embed = gnn.apply(params['gnn'], input_graphs[0])\n",
        "  params['transformer']['embedder']['input_embedding'] = (\n",
        "      params['transformer']['embedder']['input_embedding']\n",
        "      .at[placeholder_token_id]\n",
        "      .set(graph_embed)\n",
        "  )\n",
        "  # Forward pass on the input data.\n",
        "  # No attention cache is needed here.\n",
        "  logits, _ = llm.apply(\n",
        "      {'params': params['transformer']},\n",
        "      input_tokens,\n",
        "      positions,\n",
        "      None,  # Attention cache is None.\n",
        "      attention_mask,\n",
        "  )\n",
        "\n",
        "  # Exclude the last step as it does not appear in the targets.\n",
        "  logits = logits[0, :-1]\n",
        "\n",
        "  # Similarly, the first token cannot be predicted.\n",
        "  target_tokens = input_tokens[0, 1:]\n",
        "  target_mask = input_mask[0, 1:]\n",
        "\n",
        "  # Convert the target labels into one-hot encoded vectors.\n",
        "  one_hot = jax.nn.one_hot(target_tokens, logits.shape[-1])\n",
        "\n",
        "  # Don't update on unwanted tokens.\n",
        "  one_hot = one_hot * target_mask.astype(one_hot.dtype)[..., jnp.newaxis]\n",
        "\n",
        "  # Normalisation factor.\n",
        "  norm_factor = 1 / (jnp.sum(target_mask) + 1e-8)\n",
        "\n",
        "  # Return the nll loss.\n",
        "  return -jnp.sum(jax.nn.log_softmax(logits) * one_hot) * norm_factor\n",
        "\n",
        "\n",
        "@functools.partial(\n",
        "    jax.jit,\n",
        "    static_argnames=['gnn', 'llm', 'optimizer', 'pad_id', 'placeholder_token_id'],\n",
        ")\n",
        "def train_step(\n",
        "    llm: transformer_lib.Transformer,\n",
        "    gnn: nn.Module,\n",
        "    params: MutableMapping[str, Any],\n",
        "    optimizer: optax.GradientTransformation,\n",
        "    opt_state: optax.OptState,\n",
        "    pad_id: int,\n",
        "    example: TrainingInput,\n",
        "    placeholder_token_id: int,\n",
        ") -\u003e tuple[jax.Array, Params, optax.OptState]:\n",
        "  \"\"\"Train step.\n",
        "\n",
        "  Args:\n",
        "    llm: gemma transformer model.\n",
        "    gnn: gnn model.\n",
        "    params: model's input parameters.\n",
        "    optimizer: optax optimizer to use.\n",
        "    opt_state: input optimizer's state.\n",
        "    pad_id: id of the pad token.\n",
        "    example: input batch.\n",
        "    placeholder_token_id: Index in the LLM vocabulary that we are using for passing\n",
        "      graph embeddings.\n",
        "\n",
        "  Returns:\n",
        "    Training loss, updated parameters, updated optimizer state.\n",
        "  \"\"\"\n",
        "\n",
        "  # Build the position and attention mask vectors.\n",
        "  positions, attention_mask = get_attention_mask_and_positions(\n",
        "      jnp.array(example.input_tokens), pad_id\n",
        "  )\n",
        "\n",
        "  # Forward and backward passes\n",
        "  train_loss, grads = jax.value_and_grad(forward_and_loss_fn)(\n",
        "      params,\n",
        "      gnn=gnn,\n",
        "      llm=llm,\n",
        "      input_tokens=example.input_tokens,\n",
        "      input_mask=example.target_mask,\n",
        "      input_graphs=example.input_graphs,\n",
        "      positions=positions,\n",
        "      attention_mask=attention_mask,\n",
        "      placeholder_token_id=placeholder_token_id,\n",
        "  )\n",
        "\n",
        "  updates, opt_state = optimizer.update(\n",
        "      grads['gnn'], opt_state, params=params['gnn']\n",
        "  )\n",
        "  params['gnn'] = optax.apply_updates(params['gnn'], updates)\n",
        "\n",
        "  return train_loss, params, opt_state\n",
        "\n",
        "\n",
        "@functools.partial(\n",
        "    jax.jit, static_argnames=['gnn', 'llm', 'pad_id', 'placeholder_token_id']\n",
        ")\n",
        "def validation_step(\n",
        "    gnn: nn.Module,\n",
        "    llm: transformer_lib.Transformer,\n",
        "    params: MutableMapping[str, Any],\n",
        "    pad_id: int,\n",
        "    example: TrainingInput,\n",
        "    placeholder_token_id: int,\n",
        ") -\u003e jax.Array:\n",
        "  \"\"\"Validation step.\n",
        "\n",
        "  Args:\n",
        "    gnn: gnn model.\n",
        "    llm: gemma transformer model.\n",
        "    params: model's input parameters. The gnn params are stored in params['gnn']\n",
        "      and the llm params are stored in params['transformer'].\n",
        "    pad_id: id of the pad token.\n",
        "    example: input batch\n",
        "    placeholder_token_id: Index in the LLM vocabulary that we are using for passing\n",
        "      graph embeddings.\n",
        "\n",
        "  Returns:\n",
        "    Validation loss.\n",
        "  \"\"\"\n",
        "  jax_input = jax.tree.map(jnp.array, example)\n",
        "  positions, attention_mask = get_attention_mask_and_positions(\n",
        "      jax_input.input_tokens, pad_id\n",
        "  )\n",
        "  val_loss = forward_and_loss_fn(\n",
        "      params,\n",
        "      gnn=gnn,\n",
        "      llm=llm,\n",
        "      input_tokens=jax_input.input_tokens,\n",
        "      input_mask=jax_input.target_mask,\n",
        "      input_graphs=jax_input.input_graphs,\n",
        "      positions=positions,\n",
        "      attention_mask=attention_mask,\n",
        "      placeholder_token_id=placeholder_token_id,\n",
        "  )\n",
        "  return val_loss\n",
        "\n",
        "\n",
        "@chex.dataclass(frozen=True)\n",
        "class TrainingConfig:\n",
        "  learning_rate: float\n",
        "  num_epochs: int\n",
        "  eval_every_n: int\n",
        "  batch_size: int\n",
        "  max_steps: int | None = None\n",
        "\n",
        "\n",
        "def train_loop(\n",
        "    llm: transformer_lib.Transformer,\n",
        "    gnn: nn.Module,\n",
        "    train_ds: list[TrainingInput],\n",
        "    validation_ds: list[TrainingInput],\n",
        "    params: Params,\n",
        "    training_cfg: TrainingConfig,\n",
        "    vocab: spm.SentencePieceProcessor,\n",
        ") -\u003e Params:\n",
        "  \"\"\"Main training loop for GraphToken.\n",
        "\n",
        "  Args:\n",
        "    llm: Gemma transformer model.\n",
        "    gnn: gnn model.\n",
        "    train_ds: training dataset.\n",
        "    validation_ds: validation dataset.\n",
        "    params: Combined params for both the LLM and GNN. The GNN params are stored\n",
        "      in params['gnn'] and the LLM params are stored in params['transformer'].\n",
        "    training_cfg: training configuration.\n",
        "    vocab: sentence piece vocabulary.\n",
        "\n",
        "  Returns:\n",
        "    Updated model's input parameters.\n",
        "  \"\"\"\n",
        "  optimizer = optax.lion(training_cfg.learning_rate)\n",
        "  opt_state = optimizer.init(params['gnn'])\n",
        "\n",
        "  avg_loss = 0\n",
        "\n",
        "  placeholder_token_id = vocab.EncodeAsIds(PLACEHOLDER_TOKEN)\n",
        "  assert (\n",
        "      len(placeholder_token_id) == 1\n",
        "  ), f'Placeholder token multiple ids: {placeholder_token_id}'\n",
        "  placeholder_token_id = placeholder_token_id[0]\n",
        "  # A first round of validation loss\n",
        "  n_steps_eval = 0\n",
        "  eval_loss = 0\n",
        "\n",
        "  with tqdm.tqdm(range(training_cfg.num_epochs * len(train_ds))) as pbar:\n",
        "    averaged_steps = 0\n",
        "    for n_steps in pbar:\n",
        "      train_example = train_ds[n_steps % len(train_ds)]\n",
        "      train_loss, params, opt_state = train_step(\n",
        "          gnn=gnn,\n",
        "          llm=llm,\n",
        "          params=params,\n",
        "          optimizer=optimizer,\n",
        "          opt_state=opt_state,\n",
        "          pad_id=vocab.pad_id(),\n",
        "          example=train_example,\n",
        "          placeholder_token_id=placeholder_token_id,\n",
        "      )\n",
        "      averaged_steps += 1\n",
        "      avg_loss += train_loss\n",
        "      if n_steps and n_steps % training_cfg.eval_every_n == 0:\n",
        "        val_iterator = validation_ds\n",
        "        avg_loss /= averaged_steps\n",
        "        averaged_steps = 0\n",
        "        pbar.write(\n",
        "            f'STEP {n_steps} training loss: {avg_loss}'\n",
        "        )\n",
        "        avg_loss = 0\n",
        "      if (\n",
        "          training_cfg.max_steps is not None\n",
        "          and n_steps \u003e training_cfg.max_steps\n",
        "      ):\n",
        "        break\n",
        "    if averaged_steps != 0:\n",
        "      avg_loss /= averaged_steps\n",
        "      pbar.write(\n",
        "            f'STEP {n_steps} training loss: {avg_loss}'\n",
        "        )\n",
        "  return params\n",
        "\n",
        "def merge_params(llm_params, gnn_params):\n",
        "  out = {}\n",
        "  out.update(llm_params)\n",
        "  out['gnn'] = gnn_params\n",
        "  return out"
      ],
      "metadata": {
        "id": "RZKA-lU7IsZ8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train a GraphToken Model"
      ],
      "metadata": {
        "id": "NSuoM8KbnXbF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Gemma Weights"
      ],
      "metadata": {
        "id": "oknoqsTMI78J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = params_lib.load_and_format_params(ckpt_path)\n",
        "\n",
        "# Reshard params over TPU device mesh\n",
        "from jax.sharding import PartitionSpec as P\n",
        "mesh = jax.sharding.Mesh(np.array(jax.devices()).reshape(4, 2), ('x', 'y'))\n",
        "sharding = jax.sharding.NamedSharding(mesh, P('x'))\n",
        "def try_to_shard(x):\n",
        "  try:\n",
        "    return jax.device_put(x, sharding)\n",
        "  except:\n",
        "    return x\n",
        "params = jax.tree_map(try_to_shard, params)\n",
        "\n",
        "\n",
        "config_2b = transformer_lib.TransformerConfig.from_params(\n",
        "    params,\n",
        "    cache_size=128  # Number of time steps in the transformer's cache\n",
        ")\n",
        "model_2b = transformer_lib.Transformer(config=config_2b)\n",
        "\n",
        "# Load vocabulary\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "assert vocab.Load(vocab_path)"
      ],
      "metadata": {
        "id": "U1q6PiLSCTTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate some training data, for the CycleCheck task"
      ],
      "metadata": {
        "id": "NUDd2EisJHX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from talk_like_a_graph import graph_generators\n",
        "from talk_like_a_graph import graph_tasks\n",
        "random_seed = 9876\n",
        "\n",
        "train_graphs = graph_generators.generate_graphs(number_of_graphs=500,\n",
        "                         algorithm='er', # Erdos-Reyni random graphs\n",
        "                         directed=False,\n",
        "                         random_seed=random_seed)\n",
        "test_graphs = graph_generators.generate_graphs(number_of_graphs=10,\n",
        "                         algorithm='er', # Erdos-Reyni random graphs\n",
        "                         directed=False,\n",
        "                         random_seed=random_seed + 12385)\n",
        "task = graph_tasks.CycleCheck()\n",
        "train_examples = list(task.prepare_examples_dict(\n",
        "    train_graphs,\n",
        "    generator_algorithms = ['er']*len(train_graphs),\n",
        "    encoding_method='adjacency').values())\n",
        "test_examples = list(task.prepare_examples_dict(\n",
        "    test_graphs,\n",
        "    generator_algorithms = ['er']*len(test_graphs),\n",
        "    encoding_method='adjacency').values())\n",
        "padder, train_ds = graphqa_ds(vocab, train_examples, max_tokens=25)\n",
        "_, test_ds = graphqa_ds(vocab, test_examples, max_tokens=25, padder=padder)"
      ],
      "metadata": {
        "id": "93oCqfLYnXFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train GraphToken"
      ],
      "metadata": {
        "id": "dJTlxG9yUx9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gin = GIN(config_2b.embed_dim, num_hidden_layers=3, hidden_dim=4)\n",
        "key = jax.random.PRNGKey(0)\n",
        "gnn_params = gin.init(key, train_ds[0].input_graphs[0])\n",
        "\n",
        "\n",
        "train_config = TrainingConfig(\n",
        "    learning_rate=0.0001, num_epochs=3, eval_every_n=250, batch_size=1\n",
        ")\n",
        "params_learned = train_loop(\n",
        "    llm=model_2b,\n",
        "    gnn=gin,\n",
        "    train_ds=train_ds,\n",
        "    validation_ds=test_ds,\n",
        "    params=merge_params(params, gnn_params),\n",
        "    training_cfg=train_config,\n",
        "    vocab=vocab,\n",
        ")"
      ],
      "metadata": {
        "id": "Uq3ZF3v7Ivo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample outputs"
      ],
      "metadata": {
        "id": "4eaBMWhPchoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "graph_token_sampler = GraphTokenSampler(\n",
        "    params=params_learned, llm=model_2b, gnn=gin, vocab=vocab\n",
        ")\n",
        "\n",
        "\n",
        "def get_graph_qa_question(ex):\n",
        "  with_graph = ex['question']\n",
        "  q_index = with_graph.find('Q:')\n",
        "  return with_graph[q_index:]\n",
        "\n",
        "for i in range(len(test_examples)):\n",
        "  tokenized_input = test_ds[i]\n",
        "  ex = test_examples[i]\n",
        "  prompt = get_graph_qa_question(ex)\n",
        "  if i == 0:\n",
        "    display(\n",
        "        Markdown(\n",
        "            '**Prompt:** '\n",
        "            + prompt\n",
        "            + '\\n\\n'\n",
        "        )\n",
        "    )\n",
        "  llm_output = graph_token_sampler(\n",
        "      [prompt],\n",
        "      tokenized_input.input_graphs,\n",
        "      total_generation_steps=15,\n",
        "      return_logits=False,\n",
        "  ).text[0]\n",
        "  display(Markdown(f'**LLM Output:** \"{llm_output}\"'))\n",
        "  display(\n",
        "      Markdown(f\"**Ground Truth:** {ex['answer']}\")\n",
        "  )\n",
        "  display(Markdown('-' * 80))\n",
        "  print()\n",
        "\n"
      ],
      "metadata": {
        "id": "0p4CxB2Lcilw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Train a model for a different task.\n",
        "\n",
        "Take the above code and modify it for the NodeCount task.\n",
        "How does your model perform?\n",
        "\n",
        "If your kernel runs out of memory run the following code to clear the TPU memory, then re-run the code block labeled 'Load Gemma weights' and retry.\n",
        "```\n",
        "for a in jax.live_arrays():\n",
        "  a.delete()\n",
        "```"
      ],
      "metadata": {
        "id": "uuW3rA3-qKy9"
      }
    }
  ]
}
